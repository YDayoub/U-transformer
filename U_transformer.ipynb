{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtiBK6uMHUprymhQpBpaaz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDayoub/U-transformer/blob/main/U_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsHozpzlIsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c248fa32-fe0b-4661-abda-a1c6a9878222"
      },
      "source": [
        "'''\n",
        "Import required libraries\n",
        "'''\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Tuple\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrbb2GkSKiJ"
      },
      "source": [
        "# batch_size = 128          \n",
        "# test_batch_size = 128   \n",
        "epochs = 100             \n",
        "lr = 5e-4               \n",
        "seed = 42               \n",
        "h_dims = 1024\n",
        "n_heads = 16\n",
        "n_blocks = 2\n",
        "dropout = 0.2\n",
        "out_dropout = 0.4\n",
        "emb_dropout = 0.1\n",
        "clip = 0.5\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "bptt = 512\n",
        "d_model = 400"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "set_seed(seed)"
      ],
      "metadata": {
        "id": "y8wnlsR8KR5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # register_buffer is used to save and retrain parameters which don't need to train\n",
        "        self.register_buffer('pe', pe, persistent=False) \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "inWGWGwa6z-_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvxLamYx-zF"
      },
      "source": [
        "# '''\n",
        "# This code is adapted from \n",
        "# https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "# '''\n",
        "# class Embedding_with_PosEncoding(nn.Module):\n",
        "#   def __init__(self,input_dim,d_model, max_len=5000,dropout=0):\n",
        "#     '''\n",
        "#     Args:\n",
        "#       d_model: hidden space dimentionality for Embedding\n",
        "#       input_dim: input space dimentionality\n",
        "#       max_len: maximum length of an input sequence\n",
        "#       drop: probability of an element to be zeroed\n",
        "#     '''\n",
        "#     super(Embedding_with_PosEncoding,self).__init__()\n",
        "#     self.emb = nn.Embedding(input_dim,d_model)\n",
        "#     self.dropout = nn.Dropout(p=dropout)\n",
        "#     pe = torch.zeros(max_len, d_model)\n",
        "#     position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#     div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "#     pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#     pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#     pe = pe.unsqueeze(0)\n",
        "#     # register_buffer is used to save and retrain parameters which don't need to train\n",
        "#     self.register_buffer('pe', pe, persistent=False) \n",
        "#   def forward(self,x):\n",
        "#     seq_len = x.size(1)\n",
        "#     x = self.emb(x)\n",
        "#     x = self.dropout(x)\n",
        "#     x = x + self.pe[:, :seq_len]\n",
        "#     return x\n",
        "#   def get_pe(self):\n",
        "#     return self.pe"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2k37K8dSjZB"
      },
      "source": [
        "# def test_positional_encoding():\n",
        "#   batch_dim,seq_len,input_dim= (15,10,10)\n",
        "#   d_model = 100\n",
        "#   max_len =100\n",
        "#   x = torch.randint(low=0, high=10,size=(batch_dim,seq_len))\n",
        "#   pos_encoder = Embedding_with_PosEncoding(input_dim,d_model,max_len)\n",
        "#   pe = pos_encoder.get_pe()\n",
        "#   res = pos_encoder(x)\n",
        "#   assert res.shape ==  torch.Size([batch_dim,seq_len,d_model])\n",
        "#   assert pe.shape == torch.Size([1, max_len, d_model])\n",
        "# test_positional_encoding()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NzV-cFUbdz"
      },
      "source": [
        "def scaled_dot_product(query,key,values,mask=None,scale=True):\n",
        "  '''\n",
        "      Args:\n",
        "        query: tensor of queries\n",
        "        key : tensor of keys\n",
        "        value: tensor of value\n",
        "        mask (numpy.ndarray): attention-mask, used to perform self attention when required\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "  '''\n",
        "  if scale:\n",
        "    depth = query.shape[-1] ** 0.5\n",
        "  else:\n",
        "    depth = 1\n",
        "  dots = torch.matmul(query,torch.swapaxes(key,-1,-2))/depth\n",
        "  if mask is not None:\n",
        "    dots = torch.where(mask,dots,torch.full_like(dots, -9e15))\n",
        "  logsumexp = torch.logsumexp(dots, axis=-1, keepdims=True)\n",
        "  dots = torch.exp(dots - logsumexp)\n",
        "  attention = torch.matmul(dots, values)\n",
        "  return attention\n",
        "def dot_product_self_attention(q, k, v,device=device):\n",
        "  '''\n",
        "    Args:\n",
        "        q: queries.\n",
        "        k: keys.\n",
        "        v: values.\n",
        "    Returns:\n",
        "        masked dot product self attention tensor.  \n",
        "  '''\n",
        "  mask_size = q.shape[-2]\n",
        "  mask = torch.tril(torch.ones((1, mask_size, mask_size), dtype=torch.bool), diagonal=0).to(device)        \n",
        "  return scaled_dot_product(q, k, v, mask)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA0CRdjiU8H"
      },
      "source": [
        "class QKV(nn.Module):\n",
        "  '''\n",
        "  takes as input a tensor of shape (batch_size,seq_len,d_model)\n",
        "  returns:\n",
        "  three tensors q,k,v of shape (batch_size,n_heads,seq_len,d_model//n_heads)\n",
        "  '''\n",
        "\n",
        "  def __init__(self,n_heads,d_model):\n",
        "    '''\n",
        "      Args:\n",
        "        n_heads: number of heads used in multihead attention\n",
        "        d_model: hidden space dimensions\n",
        "    '''\n",
        "    assert d_model%n_heads==0,'d_models should be divisible by n_heads'\n",
        "    super(QKV,self).__init__()\n",
        "    self.qvk = nn.Linear(in_features=d_model,out_features=3*d_model)\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_heads = d_model//n_heads\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len,d_model = x.shape\n",
        "    x = self.qvk(x)\n",
        "    x = x.reshape(batch_size,seq_len,self.n_heads,3*self.d_heads)\n",
        "    x = x.permute(0,2,1,3)\n",
        "    q,k,v = x.chunk(3,dim=-1)\n",
        "    return q,k,v\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JEfLDB7oW_"
      },
      "source": [
        "def test_QKV():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  x = torch.randn(batch_dim,seq_len, d_model).to(device)\n",
        "  qkv = QKV(n_heads=n_heads,d_model=d_model).to(device)\n",
        "  q,k,v = qkv(x)\n",
        "  assert q.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert k.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert v.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "test_QKV()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CR3TtPk2wnr"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  '''\n",
        "  This class implements mulithead attention\n",
        "  '''\n",
        "  def __init__(self,d_model,causal_attention=False):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        causal_attention: boolean whether to use attention or causal attention \n",
        "    '''\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.o = nn.Linear(in_features=d_model,out_features=d_model)\n",
        "    self.causal_attention = causal_attention \n",
        "\n",
        "  def forward(self,q,k,v):\n",
        "    batch_size,n_heads,seq_len,d_heads = q.shape\n",
        "    if self.causal_attention:\n",
        "      atten =  dot_product_self_attention(q, k, v)\n",
        "    else:\n",
        "      atten = scaled_dot_product(q,k,v)\n",
        "    atten = atten.permute(0,2,1,3)\n",
        "    atten = atten.reshape(batch_size,seq_len,self.d_model)\n",
        "    res = self.o(atten)\n",
        "    return res\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTh48ZAH8mSw"
      },
      "source": [
        "def test_MultiheadAttention():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  att = MultiheadAttention(d_model,causal_attention=False).to(device)\n",
        "  causal_att = MultiheadAttention(d_model,causal_attention=True).to(device)\n",
        "  x = torch.randn(batch_dim, n_heads, seq_len,3,d_model//n_heads).to(device)\n",
        "  q,k,v = x[:,:,:,0,:],x[:,:,:,1,:],x[:,:,:,2,:]\n",
        "  o1 = att(q,k,v)\n",
        "  o2 = causal_att(q,k,v)\n",
        "  assert o1.shape ==  torch.Size([batch_dim, seq_len,d_model])\n",
        "  assert o2.shape ==  torch.Size([batch_dim,  seq_len,d_model])\n",
        "test_MultiheadAttention()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTC9-skP2ory"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  '''\n",
        "  This class implements encoder block\n",
        "  '''\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(EncoderBlock,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.qkv =  QKV(n_heads=n_heads,d_model=d_model)\n",
        "    self.attention = MultiheadAttention(d_model=d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = self.feedforward(x2)\n",
        "    x4 = self.norm2(self.dropout(x3)+x2)\n",
        "    return x4\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWmFwuHFO46"
      },
      "source": [
        "def reshape_tensor(x,n_heads):\n",
        "  '''\n",
        "    Args:\n",
        "      x: tensor of shape (batch_size,seq_len,d_model)\n",
        "      n_heads: number of heads in mutlihead attention\n",
        "    Returns:\n",
        "      reshaped tensor of shape (batch_size,n_heads,seq_len,d_model//n_heads)    \n",
        "  '''\n",
        "  batch_size,seq_len,d_model = x.shape\n",
        "  x = x.reshape(batch_size,seq_len,n_heads,d_model//n_heads)\n",
        "  x = x.permute(0,2,1,3)\n",
        "  return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  '''\n",
        "    This class implements decoder block\n",
        "  '''\n",
        "\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.qkv = QKV(n_heads,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.causal_attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0,skip_con):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.causal_attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = reshape_tensor(x2,self.n_heads)\n",
        "    skip_con = reshape_tensor(skip_con,self.n_heads)\n",
        "    x4 = self.attention(x3,skip_con,skip_con)\n",
        "    x5 = self.norm2(x2+self.dropout(x4))\n",
        "    x6 = self.feedforward(x5)\n",
        "    x7 = self.norm3(self.dropout(x6)+x5)\n",
        "    return x7"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG1NMhDKAvw"
      },
      "source": [
        "class UnetTransformer(nn.Module):\n",
        "  '''\n",
        "    This class implements unet transformer\n",
        "  '''\n",
        "  def __init__(self,n_blocks,n_tokens,n_heads,d_model,dim_feedforward,emb_dropout=0.2,\\\n",
        "               out_dropout = 0.4, dropout=0.2):\n",
        "\n",
        "    '''\n",
        "      Args:\n",
        "        n_blocks: number of encoder/decoder blocks\n",
        "        n_tokens: Dimensionality of the input space\n",
        "        n_heads: number of heads in MultiHeadAttention\n",
        "        d_model: Dimensionality of the embedding space\n",
        "        num_classes: Dimensionality of the output space\n",
        "        dim_feedforward:  Dimensionality of the hidden layer in the MLP \n",
        "\n",
        "\n",
        "    '''\n",
        "    super(UnetTransformer,self).__init__()\n",
        "    self.n_blocks = n_blocks\n",
        "    self.emb = nn.Embedding(n_tokens, d_model)\n",
        "    self.pos_enc = PositionalEncoding( d_model=d_model, dropout = emb_dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['enc_'+str(i)] = EncoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['dec_'+str(i)] = DecoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    self.output_layer = nn.Sequential( nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(out_dropout),\n",
        "            nn.Linear(d_model, n_tokens)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "    x_encoded = self.pos_enc(self.emb(x))\n",
        "    layers = vars(self)['_modules']\n",
        "    stack = [x_encoded]\n",
        "    x = layers['enc_0'](x_encoded)\n",
        "    for i in range(1,self.n_blocks):\n",
        "      stack.append(x)\n",
        "      x = layers['enc_'+str(i)](x)\n",
        "    stack.append(x)\n",
        "    x = layers['dec_0'](x,stack.pop(0))\n",
        "    for i in range(1,self.n_blocks):\n",
        "      x = layers['dec_'+str(i)](x,stack.pop(0))\n",
        "    return self.output_layer(x)\n",
        "\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset"
      ],
      "metadata": {
        "id": "BcWK--CdaBXU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "c2n815-3aGA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c629dc-a85a-4224-e3cf-6f58f41bf9a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.48M/4.48M [00:00<00:00, 28.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source: Tensor, i: int, desired_len: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "\n",
        "    seq_len = min(desired_len, source.shape[1] - 1 - i)\n",
        "    data = source[:,i:i+seq_len]\n",
        "    target = source[:,i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "get_batch(train_data,0,10)[0].shape"
      ],
      "metadata": {
        "id": "0CGLbgM1aS2m",
        "outputId": "93491f2c-93a2-4afd-b328-4dbcd135d244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "print('n_tokens {}'.format(len(vocab)))\n",
        "model = UnetTransformer(n_blocks=n_blocks,n_tokens=ntokens,\\\n",
        "                        n_heads=n_heads, d_model = d_model,dim_feedforward = h_dims,\\\n",
        "                        dropout=dropout,out_dropout = out_dropout, emb_dropout=emb_dropout).to(device)\n",
        "pytorch_total_params = sum(p.numel()\n",
        "                        for p in model.parameters() if p.requires_grad)\n",
        "print('-' * 89)\n",
        "print(\n",
        "    '#'*12+f\" Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters for {epochs:3d} epochs \"+'#'*12)\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8us_WwAIaup5",
        "outputId": "0e8fd2e7-ecfa-4c5c-f779-5b02dafe7d24"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens 33243\n",
            "-----------------------------------------------------------------------------------------\n",
            "############ Training model with 32.97M trainable parameters for 100 epochs ############\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicOpt:\n",
        "    def __init__(self, optimizer, schedular):\n",
        "        self.optimizer = optimizer\n",
        "        self.schedular = schedular\n",
        "        self._scalar = 1\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    @property\n",
        "    def lr(self):\n",
        "      return self.optimizer.param_groups[0]['lr']*self._scalar\n",
        "\n",
        "    @property\n",
        "    def scalar(self):\n",
        "        return self._scalar\n",
        "\n",
        "    @scalar.setter\n",
        "    def scalar(self, scalar):\n",
        "        self._scalar = scalar\n",
        "\n",
        "\n",
        "    def schedule_step(self, val_loss):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class linearcycleWarmup(BasicOpt):\n",
        "    def __init__(self, optimizer, schedular, *args, **kwargs):\n",
        "        super().__init__(optimizer=optimizer, schedular=schedular)\n",
        "        self.use_scheduler = True\n",
        "\n",
        "       \n",
        "    def step(self):\n",
        "        lr_s = [p['lr'] for p in self.optimizer.param_groups]\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr']  = p['lr']*self._scalar             \n",
        "        self.optimizer.step()\n",
        "        for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = lr_s[idx]\n",
        "        try:\n",
        "          if self.use_scheduler:\n",
        "            self.schedular.step()\n",
        "        except Exception as e:\n",
        "          self.use_scheduler = False\n",
        "          for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = 0.0000088\n",
        "\n",
        "\n",
        "\n",
        "    def schedule_step(self, *args):\n",
        "        pass\n",
        "steps_per_epoch = train_data.size(1)//bptt+1\n",
        "total_steps = epochs*(steps_per_epoch)\n",
        "opt_args = {\n",
        "    'lr': 0,\n",
        "    'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "}\n",
        "\n",
        "linear_args = {\n",
        "    'total_steps': total_steps,\n",
        "    'pct_start': 0.3, 'anneal_strategy': 'linear',\n",
        "    'three_phase': True, 'max_lr': 1e-3\n",
        "}\n",
        "opt = torch.optim.RAdam(model.parameters(),\n",
        "                        **opt_args)\n",
        "schedular_args = linear_args\n",
        "schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)"
      ],
      "metadata": {
        "id": "yk9NtX6gf9qw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequence_length(bptt, use_var_length):\n",
        "    if not use_var_length:\n",
        "        return bptt\n",
        "    seq_len = bptt if np.random.random() < 0.95 else bptt // 2\n",
        "    seq_len = max(5, int(np.random.normal(seq_len, 5)))\n",
        "    seq_len = min(seq_len, bptt + 30)\n",
        "    return seq_len"
      ],
      "metadata": {
        "id": "Ph4OuVfI8gG5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = train_data.shape[1] // bptt\n",
        "    i, batch = 0, 0\n",
        "    while i < train_data.size(1) - 1 - 1:\n",
        "        data, targets = get_batch(train_data, i, get_sequence_length(bptt, use_var_length=True))\n",
        "        batch_size = data.size(1)\n",
        "        # if batch_size != bptt:  # only on last batch\n",
        "        #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data)\n",
        "        scale_lr = batch_size/bptt\n",
        "        optimizer.scalar = scale_lr\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = optimizer.lr\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}| seq_len {batch_size:5d}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        batch += 1\n",
        "        i += batch_size\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(1) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i, bptt)\n",
        "            batch_size = data.size(1)\n",
        "            # if batch_size != bptt:\n",
        "            #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (eval_data.size(1) - 1)"
      ],
      "metadata": {
        "id": "Gspv87NsaWha"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHvZbtTJaa-4",
        "outputId": "cff7bc23-2fe9-4270-f79e-54e29f6fd87c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  434 batches | lr 0.000055 | ms/batch 56.33 | loss  9.41 | ppl 12263.40| seq_len   258\n",
            "| epoch   1 |   400/  434 batches | lr 0.000068 | ms/batch 55.32 | loss  7.38 | ppl  1600.50| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 26.04s | valid loss  5.95 | valid ppl   385.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  434 batches | lr 0.000089 | ms/batch 55.68 | loss  6.48 | ppl   653.10| seq_len   260\n",
            "| epoch   2 |   400/  434 batches | lr 0.000103 | ms/batch 55.99 | loss  6.26 | ppl   524.52| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 25.97s | valid loss  5.53 | valid ppl   252.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  434 batches | lr 0.000122 | ms/batch 56.27 | loss  6.09 | ppl   441.71| seq_len   257\n",
            "| epoch   3 |   400/  434 batches | lr 0.000134 | ms/batch 55.86 | loss  5.93 | ppl   375.92| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 26.04s | valid loss  5.31 | valid ppl   201.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  434 batches | lr 0.000155 | ms/batch 55.85 | loss  5.84 | ppl   343.93| seq_len   257\n",
            "| epoch   4 |   400/  434 batches | lr 0.000169 | ms/batch 55.96 | loss  5.73 | ppl   306.98| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 26.02s | valid loss  5.16 | valid ppl   174.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  434 batches | lr 0.000192 | ms/batch 56.16 | loss  5.67 | ppl   289.30| seq_len   262\n",
            "| epoch   5 |   400/  434 batches | lr 0.000192 | ms/batch 55.58 | loss  5.57 | ppl   262.82| seq_len   243\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 26.03s | valid loss  5.05 | valid ppl   156.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  434 batches | lr 0.000215 | ms/batch 56.56 | loss  5.52 | ppl   250.47| seq_len   250\n",
            "| epoch   6 |   400/  434 batches | lr 0.000232 | ms/batch 56.21 | loss  5.44 | ppl   230.86| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 25.93s | valid loss  4.97 | valid ppl   143.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  434 batches | lr 0.000251 | ms/batch 56.30 | loss  5.41 | ppl   223.49| seq_len   254\n",
            "| epoch   7 |   400/  434 batches | lr 0.000264 | ms/batch 56.44 | loss  5.34 | ppl   208.03| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 26.02s | valid loss  4.90 | valid ppl   133.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  434 batches | lr 0.000287 | ms/batch 56.81 | loss  5.31 | ppl   203.24| seq_len   257\n",
            "| epoch   8 |   400/  434 batches | lr 0.000301 | ms/batch 55.86 | loss  5.25 | ppl   191.32| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 26.02s | valid loss  4.84 | valid ppl   126.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  434 batches | lr 0.000315 | ms/batch 56.37 | loss  5.24 | ppl   188.55| seq_len   253\n",
            "| epoch   9 |   400/  434 batches | lr 0.000330 | ms/batch 56.11 | loss  5.19 | ppl   178.63| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 26.03s | valid loss  4.80 | valid ppl   121.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  434 batches | lr 0.000353 | ms/batch 56.96 | loss  5.17 | ppl   176.63| seq_len   257\n",
            "| epoch  10 |   400/  434 batches | lr 0.000364 | ms/batch 56.29 | loss  5.13 | ppl   168.57| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 25.95s | valid loss  4.77 | valid ppl   117.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/  434 batches | lr 0.000377 | ms/batch 56.39 | loss  5.13 | ppl   168.57| seq_len   251\n",
            "| epoch  11 |   400/  434 batches | lr 0.000404 | ms/batch 55.47 | loss  5.09 | ppl   162.83| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 25.94s | valid loss  4.72 | valid ppl   112.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/  434 batches | lr 0.000417 | ms/batch 55.19 | loss  5.09 | ppl   162.67| seq_len   256\n",
            "| epoch  12 |   400/  434 batches | lr 0.000422 | ms/batch 56.31 | loss  5.06 | ppl   157.68| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 25.99s | valid loss  4.69 | valid ppl   108.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/  434 batches | lr 0.000447 | ms/batch 55.44 | loss  5.06 | ppl   158.05| seq_len   254\n",
            "| epoch  13 |   400/  434 batches | lr 0.000447 | ms/batch 55.74 | loss  5.04 | ppl   153.76| seq_len   246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 25.88s | valid loss  4.66 | valid ppl   105.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/  434 batches | lr 0.000489 | ms/batch 56.25 | loss  5.04 | ppl   154.41| seq_len   259\n",
            "| epoch  14 |   400/  434 batches | lr 0.000498 | ms/batch 56.49 | loss  5.02 | ppl   150.82| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 26.03s | valid loss  4.64 | valid ppl   103.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/  434 batches | lr 0.000523 | ms/batch 57.02 | loss  5.02 | ppl   150.98| seq_len   259\n",
            "| epoch  15 |   400/  434 batches | lr 0.000527 | ms/batch 55.58 | loss  5.00 | ppl   148.91| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 26.02s | valid loss  4.63 | valid ppl   102.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/  434 batches | lr 0.000550 | ms/batch 56.48 | loss  5.00 | ppl   147.81| seq_len   256\n",
            "| epoch  16 |   400/  434 batches | lr 0.000560 | ms/batch 55.86 | loss  4.98 | ppl   145.62| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 26.02s | valid loss  4.60 | valid ppl    99.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/  434 batches | lr 0.000587 | ms/batch 56.84 | loss  4.97 | ppl   144.26| seq_len   258\n",
            "| epoch  17 |   400/  434 batches | lr 0.000604 | ms/batch 56.23 | loss  4.96 | ppl   142.00| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 25.99s | valid loss  4.56 | valid ppl    95.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/  434 batches | lr 0.000608 | ms/batch 57.10 | loss  4.94 | ppl   140.24| seq_len   253\n",
            "| epoch  18 |   400/  434 batches | lr 0.000633 | ms/batch 55.60 | loss  4.93 | ppl   137.97| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 26.02s | valid loss  4.54 | valid ppl    93.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/  434 batches | lr 0.000666 | ms/batch 56.72 | loss  4.91 | ppl   135.71| seq_len   263\n",
            "| epoch  19 |   400/  434 batches | lr 0.000679 | ms/batch 55.87 | loss  4.89 | ppl   133.51| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 25.93s | valid loss  4.51 | valid ppl    90.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/  434 batches | lr 0.000689 | ms/batch 56.22 | loss  4.89 | ppl   132.50| seq_len   259\n",
            "| epoch  20 |   400/  434 batches | lr 0.000690 | ms/batch 55.64 | loss  4.87 | ppl   130.39| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 25.99s | valid loss  4.48 | valid ppl    88.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/  434 batches | lr 0.000717 | ms/batch 55.92 | loss  4.86 | ppl   129.52| seq_len   257\n",
            "| epoch  21 |   400/  434 batches | lr 0.000746 | ms/batch 56.05 | loss  4.85 | ppl   127.23| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 26.01s | valid loss  4.47 | valid ppl    86.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/  434 batches | lr 0.000768 | ms/batch 56.36 | loss  4.84 | ppl   126.55| seq_len   263\n",
            "| epoch  22 |   400/  434 batches | lr 0.000771 | ms/batch 56.82 | loss  4.82 | ppl   123.72| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 26.01s | valid loss  4.43 | valid ppl    84.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/  434 batches | lr 0.000765 | ms/batch 56.11 | loss  4.82 | ppl   123.59| seq_len   251\n",
            "| epoch  23 |   400/  434 batches | lr 0.000773 | ms/batch 56.11 | loss  4.80 | ppl   121.91| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 25.92s | valid loss  4.41 | valid ppl    82.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/  434 batches | lr 0.000787 | ms/batch 56.66 | loss  4.80 | ppl   121.54| seq_len   248\n",
            "| epoch  24 |   400/  434 batches | lr 0.000850 | ms/batch 56.21 | loss  4.79 | ppl   120.20| seq_len   263\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 26.05s | valid loss  4.40 | valid ppl    81.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/  434 batches | lr 0.000832 | ms/batch 56.67 | loss  4.78 | ppl   119.53| seq_len   252\n",
            "| epoch  25 |   400/  434 batches | lr 0.000867 | ms/batch 55.37 | loss  4.78 | ppl   118.86| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 25.94s | valid loss  4.38 | valid ppl    79.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/  434 batches | lr 0.000889 | ms/batch 56.88 | loss  4.78 | ppl   118.89| seq_len   259\n",
            "| epoch  26 |   400/  434 batches | lr 0.000872 | ms/batch 55.80 | loss  4.77 | ppl   117.96| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 25.97s | valid loss  4.36 | valid ppl    78.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/  434 batches | lr 0.000897 | ms/batch 55.57 | loss  4.77 | ppl   117.42| seq_len   252\n",
            "| epoch  27 |   400/  434 batches | lr 0.000912 | ms/batch 55.62 | loss  4.76 | ppl   116.95| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 26.04s | valid loss  4.36 | valid ppl    78.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/  434 batches | lr 0.000476 | ms/batch 55.50 | loss  4.76 | ppl   116.54| seq_len   129\n",
            "| epoch  28 |   400/  434 batches | lr 0.000948 | ms/batch 56.08 | loss  4.75 | ppl   115.05| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 26.02s | valid loss  4.35 | valid ppl    77.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/  434 batches | lr 0.000982 | ms/batch 56.93 | loss  4.74 | ppl   114.64| seq_len   257\n",
            "| epoch  29 |   400/  434 batches | lr 0.000946 | ms/batch 57.19 | loss  4.73 | ppl   113.01| seq_len   244\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 25.94s | valid loss  4.33 | valid ppl    75.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/  434 batches | lr 0.000982 | ms/batch 55.79 | loss  4.73 | ppl   113.08| seq_len   254\n",
            "| epoch  30 |   400/  434 batches | lr 0.000952 | ms/batch 55.71 | loss  4.71 | ppl   111.33| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 25.89s | valid loss  4.33 | valid ppl    76.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/  434 batches | lr 0.000960 | ms/batch 57.18 | loss  4.70 | ppl   109.91| seq_len   257\n",
            "| epoch  31 |   400/  434 batches | lr 0.000909 | ms/batch 55.31 | loss  4.69 | ppl   108.66| seq_len   247\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 26.00s | valid loss  4.31 | valid ppl    74.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/  434 batches | lr 0.000938 | ms/batch 56.51 | loss  4.67 | ppl   106.50| seq_len   260\n",
            "| epoch  32 |   400/  434 batches | lr 0.000884 | ms/batch 56.68 | loss  4.65 | ppl   104.94| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 26.02s | valid loss  4.29 | valid ppl    73.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/  434 batches | lr 0.000842 | ms/batch 57.12 | loss  4.64 | ppl   103.41| seq_len   242\n",
            "| epoch  33 |   400/  434 batches | lr 0.000869 | ms/batch 56.30 | loss  4.63 | ppl   102.16| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 25.85s | valid loss  4.28 | valid ppl    72.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/  434 batches | lr 0.000862 | ms/batch 56.40 | loss  4.61 | ppl   100.53| seq_len   257\n",
            "| epoch  34 |   400/  434 batches | lr 0.000834 | ms/batch 55.52 | loss  4.61 | ppl   100.35| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 26.00s | valid loss  4.27 | valid ppl    71.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/  434 batches | lr 0.000790 | ms/batch 56.26 | loss  4.60 | ppl    99.32| seq_len   245\n",
            "| epoch  35 |   400/  434 batches | lr 0.000804 | ms/batch 56.36 | loss  4.58 | ppl    97.89| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 25.98s | valid loss  4.26 | valid ppl    70.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/  434 batches | lr 0.000743 | ms/batch 56.56 | loss  4.57 | ppl    96.59| seq_len   240\n",
            "| epoch  36 |   400/  434 batches | lr 0.000796 | ms/batch 55.44 | loss  4.56 | ppl    95.72| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 25.98s | valid loss  4.24 | valid ppl    69.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/  434 batches | lr 0.000742 | ms/batch 56.44 | loss  4.55 | ppl    94.41| seq_len   250\n",
            "| epoch  37 |   400/  434 batches | lr 0.000759 | ms/batch 55.95 | loss  4.54 | ppl    93.37| seq_len   261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 26.04s | valid loss  4.23 | valid ppl    69.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/  434 batches | lr 0.000724 | ms/batch 56.98 | loss  4.53 | ppl    92.35| seq_len   255\n",
            "| epoch  38 |   400/  434 batches | lr 0.000704 | ms/batch 55.99 | loss  4.51 | ppl    91.14| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 25.93s | valid loss  4.22 | valid ppl    68.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/  434 batches | lr 0.000675 | ms/batch 57.30 | loss  4.51 | ppl    90.76| seq_len   249\n",
            "| epoch  39 |   400/  434 batches | lr 0.000677 | ms/batch 55.24 | loss  4.50 | ppl    90.34| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 26.02s | valid loss  4.21 | valid ppl    67.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/  434 batches | lr 0.000331 | ms/batch 57.03 | loss  4.49 | ppl    89.46| seq_len   128\n",
            "| epoch  40 |   400/  434 batches | lr 0.000641 | ms/batch 56.19 | loss  4.48 | ppl    88.35| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 26.05s | valid loss  4.20 | valid ppl    66.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/  434 batches | lr 0.000626 | ms/batch 56.68 | loss  4.48 | ppl    88.45| seq_len   255\n",
            "| epoch  41 |   400/  434 batches | lr 0.000618 | ms/batch 56.44 | loss  4.47 | ppl    87.52| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 26.09s | valid loss  4.20 | valid ppl    66.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/  434 batches | lr 0.000577 | ms/batch 56.36 | loss  4.46 | ppl    86.30| seq_len   248\n",
            "| epoch  42 |   400/  434 batches | lr 0.000603 | ms/batch 56.37 | loss  4.45 | ppl    85.45| seq_len   266\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 26.00s | valid loss  4.19 | valid ppl    65.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/  434 batches | lr 0.000547 | ms/batch 56.34 | loss  4.44 | ppl    85.15| seq_len   249\n",
            "| epoch  43 |   400/  434 batches | lr 0.000548 | ms/batch 55.94 | loss  4.43 | ppl    84.30| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 26.01s | valid loss  4.17 | valid ppl    64.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/  434 batches | lr 0.000509 | ms/batch 56.59 | loss  4.42 | ppl    83.42| seq_len   246\n",
            "| epoch  44 |   400/  434 batches | lr 0.000529 | ms/batch 56.24 | loss  4.42 | ppl    82.82| seq_len   263\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 26.01s | valid loss  4.17 | valid ppl    64.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/  434 batches | lr 0.000504 | ms/batch 57.80 | loss  4.40 | ppl    81.68| seq_len   260\n",
            "| epoch  45 |   400/  434 batches | lr 0.000474 | ms/batch 56.50 | loss  4.39 | ppl    80.98| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 26.03s | valid loss  4.16 | valid ppl    64.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/  434 batches | lr 0.000468 | ms/batch 57.35 | loss  4.39 | ppl    80.50| seq_len   258\n",
            "| epoch  46 |   400/  434 batches | lr 0.000448 | ms/batch 56.34 | loss  4.38 | ppl    79.74| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 26.01s | valid loss  4.15 | valid ppl    63.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/  434 batches | lr 0.000440 | ms/batch 56.37 | loss  4.37 | ppl    78.92| seq_len   261\n",
            "| epoch  47 |   400/  434 batches | lr 0.000409 | ms/batch 56.11 | loss  4.37 | ppl    78.93| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 25.95s | valid loss  4.15 | valid ppl    63.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/  434 batches | lr 0.000396 | ms/batch 55.86 | loss  4.36 | ppl    78.02| seq_len   254\n",
            "| epoch  48 |   400/  434 batches | lr 0.000393 | ms/batch 56.65 | loss  4.35 | ppl    77.26| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 25.94s | valid loss  4.15 | valid ppl    63.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/  434 batches | lr 0.000369 | ms/batch 56.82 | loss  4.33 | ppl    76.08| seq_len   258\n",
            "| epoch  49 |   400/  434 batches | lr 0.000348 | ms/batch 55.80 | loss  4.33 | ppl    76.10| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 25.93s | valid loss  4.14 | valid ppl    62.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/  434 batches | lr 0.000328 | ms/batch 56.93 | loss  4.32 | ppl    74.82| seq_len   252\n",
            "| epoch  50 |   400/  434 batches | lr 0.000321 | ms/batch 56.45 | loss  4.31 | ppl    74.55| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 25.98s | valid loss  4.12 | valid ppl    61.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   200/  434 batches | lr 0.000308 | ms/batch 56.96 | loss  4.30 | ppl    73.55| seq_len   262\n",
            "| epoch  51 |   400/  434 batches | lr 0.000285 | ms/batch 56.06 | loss  4.30 | ppl    73.34| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 26.00s | valid loss  4.11 | valid ppl    61.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   200/  434 batches | lr 0.000265 | ms/batch 56.24 | loss  4.28 | ppl    72.38| seq_len   253\n",
            "| epoch  52 |   400/  434 batches | lr 0.000122 | ms/batch 55.84 | loss  4.28 | ppl    72.22| seq_len   123\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 26.05s | valid loss  4.11 | valid ppl    60.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |   200/  434 batches | lr 0.000247 | ms/batch 55.68 | loss  4.27 | ppl    71.43| seq_len   270\n",
            "| epoch  53 |   400/  434 batches | lr 0.000220 | ms/batch 55.50 | loss  4.27 | ppl    71.62| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 25.97s | valid loss  4.10 | valid ppl    60.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   200/  434 batches | lr 0.000197 | ms/batch 57.28 | loss  4.25 | ppl    69.91| seq_len   250\n",
            "| epoch  54 |   400/  434 batches | lr 0.000190 | ms/batch 55.82 | loss  4.25 | ppl    70.41| seq_len   260\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 26.08s | valid loss  4.10 | valid ppl    60.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |   200/  434 batches | lr 0.000170 | ms/batch 57.21 | loss  4.23 | ppl    68.66| seq_len   258\n",
            "| epoch  55 |   400/  434 batches | lr 0.000153 | ms/batch 56.67 | loss  4.23 | ppl    68.99| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 26.10s | valid loss  4.10 | valid ppl    60.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |   200/  434 batches | lr 0.000136 | ms/batch 56.45 | loss  4.22 | ppl    67.70| seq_len   257\n",
            "| epoch  56 |   400/  434 batches | lr 0.000059 | ms/batch 56.22 | loss  4.22 | ppl    68.10| seq_len   124\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 26.06s | valid loss  4.09 | valid ppl    59.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |   200/  434 batches | lr 0.000107 | ms/batch 56.72 | loss  4.19 | ppl    66.19| seq_len   266\n",
            "| epoch  57 |   400/  434 batches | lr 0.000088 | ms/batch 55.07 | loss  4.21 | ppl    67.06| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 26.09s | valid loss  4.08 | valid ppl    59.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |   200/  434 batches | lr 0.000070 | ms/batch 56.10 | loss  4.18 | ppl    65.63| seq_len   256\n",
            "| epoch  58 |   400/  434 batches | lr 0.000056 | ms/batch 57.02 | loss  4.18 | ppl    65.44| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 26.05s | valid loss  4.08 | valid ppl    59.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |   200/  434 batches | lr 0.000040 | ms/batch 56.39 | loss  4.17 | ppl    64.47| seq_len   256\n",
            "| epoch  59 |   400/  434 batches | lr 0.000039 | ms/batch 56.41 | loss  4.17 | ppl    64.76| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 26.07s | valid loss  4.07 | valid ppl    58.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |   200/  434 batches | lr 0.000038 | ms/batch 56.97 | loss  4.16 | ppl    63.89| seq_len   249\n",
            "| epoch  60 |   400/  434 batches | lr 0.000039 | ms/batch 56.04 | loss  4.17 | ppl    64.47| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 25.98s | valid loss  4.07 | valid ppl    58.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |   200/  434 batches | lr 0.000037 | ms/batch 56.10 | loss  4.15 | ppl    63.60| seq_len   248\n",
            "| epoch  61 |   400/  434 batches | lr 0.000037 | ms/batch 55.33 | loss  4.16 | ppl    64.19| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 26.01s | valid loss  4.07 | valid ppl    58.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |   200/  434 batches | lr 0.000018 | ms/batch 56.03 | loss  4.15 | ppl    63.34| seq_len   127\n",
            "| epoch  62 |   400/  434 batches | lr 0.000035 | ms/batch 56.37 | loss  4.15 | ppl    63.68| seq_len   246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 25.94s | valid loss  4.07 | valid ppl    58.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |   200/  434 batches | lr 0.000035 | ms/batch 55.23 | loss  4.14 | ppl    62.97| seq_len   253\n",
            "| epoch  63 |   400/  434 batches | lr 0.000035 | ms/batch 55.68 | loss  4.15 | ppl    63.69| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 25.98s | valid loss  4.07 | valid ppl    58.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |   200/  434 batches | lr 0.000034 | ms/batch 56.04 | loss  4.14 | ppl    62.79| seq_len   252\n",
            "| epoch  64 |   400/  434 batches | lr 0.000018 | ms/batch 55.65 | loss  4.15 | ppl    63.65| seq_len   133\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 25.93s | valid loss  4.07 | valid ppl    58.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |   200/  434 batches | lr 0.000033 | ms/batch 56.92 | loss  4.14 | ppl    62.69| seq_len   254\n",
            "| epoch  65 |   400/  434 batches | lr 0.000033 | ms/batch 56.33 | loss  4.15 | ppl    63.33| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 26.02s | valid loss  4.07 | valid ppl    58.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |   200/  434 batches | lr 0.000033 | ms/batch 56.27 | loss  4.13 | ppl    62.35| seq_len   256\n",
            "| epoch  66 |   400/  434 batches | lr 0.000032 | ms/batch 56.16 | loss  4.15 | ppl    63.16| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 25.97s | valid loss  4.07 | valid ppl    58.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |   200/  434 batches | lr 0.000032 | ms/batch 55.63 | loss  4.13 | ppl    62.43| seq_len   258\n",
            "| epoch  67 |   400/  434 batches | lr 0.000031 | ms/batch 56.23 | loss  4.14 | ppl    62.90| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 25.94s | valid loss  4.07 | valid ppl    58.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |   200/  434 batches | lr 0.000030 | ms/batch 56.62 | loss  4.13 | ppl    61.90| seq_len   249\n",
            "| epoch  68 |   400/  434 batches | lr 0.000031 | ms/batch 55.98 | loss  4.14 | ppl    62.87| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 25.98s | valid loss  4.07 | valid ppl    58.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |   200/  434 batches | lr 0.000030 | ms/batch 56.54 | loss  4.12 | ppl    61.68| seq_len   263\n",
            "| epoch  69 |   400/  434 batches | lr 0.000030 | ms/batch 56.20 | loss  4.14 | ppl    62.73| seq_len   261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 26.03s | valid loss  4.07 | valid ppl    58.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |   200/  434 batches | lr 0.000029 | ms/batch 55.88 | loss  4.12 | ppl    61.82| seq_len   259\n",
            "| epoch  70 |   400/  434 batches | lr 0.000014 | ms/batch 56.31 | loss  4.14 | ppl    62.75| seq_len   124\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 25.99s | valid loss  4.07 | valid ppl    58.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |   200/  434 batches | lr 0.000028 | ms/batch 57.07 | loss  4.12 | ppl    61.49| seq_len   258\n",
            "| epoch  71 |   400/  434 batches | lr 0.000028 | ms/batch 55.59 | loss  4.13 | ppl    62.41| seq_len   260\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 25.98s | valid loss  4.07 | valid ppl    58.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |   200/  434 batches | lr 0.000027 | ms/batch 56.85 | loss  4.12 | ppl    61.49| seq_len   256\n",
            "| epoch  72 |   400/  434 batches | lr 0.000026 | ms/batch 55.89 | loss  4.13 | ppl    62.38| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 26.03s | valid loss  4.06 | valid ppl    58.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |   200/  434 batches | lr 0.000025 | ms/batch 56.63 | loss  4.12 | ppl    61.57| seq_len   254\n",
            "| epoch  73 |   400/  434 batches | lr 0.000025 | ms/batch 56.85 | loss  4.13 | ppl    62.05| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 25.95s | valid loss  4.06 | valid ppl    58.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |   200/  434 batches | lr 0.000025 | ms/batch 56.63 | loss  4.12 | ppl    61.36| seq_len   257\n",
            "| epoch  74 |   400/  434 batches | lr 0.000024 | ms/batch 56.03 | loss  4.13 | ppl    62.30| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 26.03s | valid loss  4.06 | valid ppl    58.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |   200/  434 batches | lr 0.000024 | ms/batch 56.43 | loss  4.12 | ppl    61.26| seq_len   261\n",
            "| epoch  75 |   400/  434 batches | lr 0.000011 | ms/batch 56.01 | loss  4.13 | ppl    62.30| seq_len   125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 26.01s | valid loss  4.06 | valid ppl    58.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |   200/  434 batches | lr 0.000023 | ms/batch 56.47 | loss  4.11 | ppl    61.21| seq_len   261\n",
            "| epoch  76 |   400/  434 batches | lr 0.000022 | ms/batch 55.78 | loss  4.13 | ppl    62.06| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 26.07s | valid loss  4.06 | valid ppl    58.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |   200/  434 batches | lr 0.000022 | ms/batch 56.70 | loss  4.11 | ppl    61.03| seq_len   261\n",
            "| epoch  77 |   400/  434 batches | lr 0.000021 | ms/batch 56.21 | loss  4.12 | ppl    61.71| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 26.02s | valid loss  4.06 | valid ppl    58.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |   200/  434 batches | lr 0.000020 | ms/batch 56.32 | loss  4.11 | ppl    60.85| seq_len   252\n",
            "| epoch  78 |   400/  434 batches | lr 0.000020 | ms/batch 55.33 | loss  4.12 | ppl    61.81| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 25.99s | valid loss  4.06 | valid ppl    58.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |   200/  434 batches | lr 0.000019 | ms/batch 56.68 | loss  4.11 | ppl    60.93| seq_len   257\n",
            "| epoch  79 |   400/  434 batches | lr 0.000019 | ms/batch 55.76 | loss  4.13 | ppl    62.01| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 26.01s | valid loss  4.06 | valid ppl    57.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |   200/  434 batches | lr 0.000018 | ms/batch 55.52 | loss  4.11 | ppl    61.10| seq_len   251\n",
            "| epoch  80 |   400/  434 batches | lr 0.000018 | ms/batch 56.13 | loss  4.12 | ppl    61.85| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 26.04s | valid loss  4.06 | valid ppl    57.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |   200/  434 batches | lr 0.000009 | ms/batch 55.95 | loss  4.11 | ppl    61.00| seq_len   131\n",
            "| epoch  81 |   400/  434 batches | lr 0.000017 | ms/batch 55.55 | loss  4.12 | ppl    61.83| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 26.00s | valid loss  4.06 | valid ppl    57.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |   200/  434 batches | lr 0.000016 | ms/batch 57.03 | loss  4.11 | ppl    60.91| seq_len   252\n",
            "| epoch  82 |   400/  434 batches | lr 0.000015 | ms/batch 56.18 | loss  4.12 | ppl    61.69| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 26.04s | valid loss  4.06 | valid ppl    57.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |   200/  434 batches | lr 0.000015 | ms/batch 56.41 | loss  4.11 | ppl    60.78| seq_len   254\n",
            "| epoch  83 |   400/  434 batches | lr 0.000015 | ms/batch 56.11 | loss  4.12 | ppl    61.64| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 25.99s | valid loss  4.06 | valid ppl    57.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |   200/  434 batches | lr 0.000014 | ms/batch 55.12 | loss  4.11 | ppl    61.18| seq_len   253\n",
            "| epoch  84 |   400/  434 batches | lr 0.000014 | ms/batch 55.43 | loss  4.12 | ppl    61.62| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 25.94s | valid loss  4.06 | valid ppl    57.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |   200/  434 batches | lr 0.000013 | ms/batch 56.84 | loss  4.11 | ppl    60.69| seq_len   255\n",
            "| epoch  85 |   400/  434 batches | lr 0.000013 | ms/batch 55.86 | loss  4.12 | ppl    61.55| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 26.02s | valid loss  4.06 | valid ppl    57.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |   200/  434 batches | lr 0.000012 | ms/batch 56.77 | loss  4.11 | ppl    60.77| seq_len   256\n",
            "| epoch  86 |   400/  434 batches | lr 0.000011 | ms/batch 56.45 | loss  4.11 | ppl    61.14| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 25.93s | valid loss  4.05 | valid ppl    57.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |   200/  434 batches | lr 0.000011 | ms/batch 55.97 | loss  4.10 | ppl    60.64| seq_len   264\n",
            "| epoch  87 |   400/  434 batches | lr 0.000010 | ms/batch 56.58 | loss  4.11 | ppl    61.21| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 25.92s | valid loss  4.05 | valid ppl    57.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |   200/  434 batches | lr 0.000010 | ms/batch 56.91 | loss  4.10 | ppl    60.57| seq_len   252\n",
            "| epoch  88 |   400/  434 batches | lr 0.000009 | ms/batch 55.51 | loss  4.12 | ppl    61.30| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 25.91s | valid loss  4.05 | valid ppl    57.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |   200/  434 batches | lr 0.000009 | ms/batch 55.96 | loss  4.10 | ppl    60.60| seq_len   252\n",
            "| epoch  89 |   400/  434 batches | lr 0.000009 | ms/batch 55.14 | loss  4.12 | ppl    61.49| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 26.04s | valid loss  4.05 | valid ppl    57.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |   200/  434 batches | lr 0.000008 | ms/batch 56.32 | loss  4.10 | ppl    60.60| seq_len   243\n",
            "| epoch  90 |   400/  434 batches | lr 0.000007 | ms/batch 55.73 | loss  4.12 | ppl    61.45| seq_len   244\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 26.00s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |   200/  434 batches | lr 0.000004 | ms/batch 56.28 | loss  4.10 | ppl    60.31| seq_len   132\n",
            "| epoch  91 |   400/  434 batches | lr 0.000007 | ms/batch 56.37 | loss  4.11 | ppl    61.13| seq_len   263\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 26.04s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |   200/  434 batches | lr 0.000006 | ms/batch 56.95 | loss  4.10 | ppl    60.29| seq_len   246\n",
            "| epoch  92 |   400/  434 batches | lr 0.000006 | ms/batch 56.12 | loss  4.11 | ppl    61.11| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 25.95s | valid loss  4.05 | valid ppl    57.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |   200/  434 batches | lr 0.000002 | ms/batch 56.02 | loss  4.10 | ppl    60.32| seq_len   121\n",
            "| epoch  93 |   400/  434 batches | lr 0.000004 | ms/batch 57.03 | loss  4.11 | ppl    60.82| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 26.02s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |   200/  434 batches | lr 0.000004 | ms/batch 56.82 | loss  4.10 | ppl    60.35| seq_len   262\n",
            "| epoch  94 |   400/  434 batches | lr 0.000003 | ms/batch 55.46 | loss  4.11 | ppl    61.00| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 26.01s | valid loss  4.05 | valid ppl    57.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |   200/  434 batches | lr 0.000003 | ms/batch 55.46 | loss  4.10 | ppl    60.36| seq_len   250\n",
            "| epoch  95 |   400/  434 batches | lr 0.000002 | ms/batch 55.56 | loss  4.11 | ppl    61.18| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 26.05s | valid loss  4.05 | valid ppl    57.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |   200/  434 batches | lr 0.000001 | ms/batch 56.74 | loss  4.10 | ppl    60.07| seq_len   133\n",
            "| epoch  96 |   400/  434 batches | lr 0.000001 | ms/batch 57.02 | loss  4.11 | ppl    60.73| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 26.06s | valid loss  4.05 | valid ppl    57.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |   200/  434 batches | lr 0.000000 | ms/batch 57.43 | loss  4.09 | ppl    60.01| seq_len   130\n",
            "| epoch  97 |   400/  434 batches | lr 0.000000 | ms/batch 56.77 | loss  4.10 | ppl    60.51| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 26.01s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |   200/  434 batches | lr 0.000001 | ms/batch 55.87 | loss  4.09 | ppl    59.99| seq_len   251\n",
            "| epoch  98 |   400/  434 batches | lr 0.000001 | ms/batch 56.37 | loss  4.10 | ppl    60.62| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 26.07s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |   200/  434 batches | lr 0.000001 | ms/batch 57.38 | loss  4.10 | ppl    60.15| seq_len   263\n",
            "| epoch  99 |   400/  434 batches | lr 0.000001 | ms/batch 55.92 | loss  4.11 | ppl    60.87| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 26.04s | valid loss  4.05 | valid ppl    57.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |   200/  434 batches | lr 0.000001 | ms/batch 56.80 | loss  4.09 | ppl    60.04| seq_len   262\n",
            "| epoch 100 |   400/  434 batches | lr 0.000001 | ms/batch 56.69 | loss  4.10 | ppl    60.56| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 26.04s | valid loss  4.05 | valid ppl    57.55\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('-' * 89)\n",
        "print(f'test loss {test_loss:5.5f} | test ppl {test_ppl:8.5f}')\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "id": "1Zgbkytp9Gqs",
        "outputId": "aaf7a8c4-2b2f-41ed-9bb2-72af06fa0497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "test loss 3.93939 | test ppl 51.38716\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "B9PEwV1-L9wB",
        "outputId": "12bec5dc-6be9-48c8-a558-c8e246272635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UnetTransformer(\n",
              "  (emb): Embedding(33243, 400)\n",
              "  (pos_enc): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (enc_0): EncoderBlock(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (qkv): QKV(\n",
              "      (qvk): Linear(in_features=400, out_features=1200, bias=True)\n",
              "    )\n",
              "    (attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (feedforward): Sequential(\n",
              "      (0): Linear(in_features=400, out_features=1024, bias=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Linear(in_features=1024, out_features=400, bias=True)\n",
              "    )\n",
              "    (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (enc_1): EncoderBlock(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (qkv): QKV(\n",
              "      (qvk): Linear(in_features=400, out_features=1200, bias=True)\n",
              "    )\n",
              "    (attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (feedforward): Sequential(\n",
              "      (0): Linear(in_features=400, out_features=1024, bias=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Linear(in_features=1024, out_features=400, bias=True)\n",
              "    )\n",
              "    (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dec_0): DecoderBlock(\n",
              "    (qkv): QKV(\n",
              "      (qvk): Linear(in_features=400, out_features=1200, bias=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (causal_attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (feedforward): Sequential(\n",
              "      (0): Linear(in_features=400, out_features=1024, bias=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Linear(in_features=1024, out_features=400, bias=True)\n",
              "    )\n",
              "    (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (dec_1): DecoderBlock(\n",
              "    (qkv): QKV(\n",
              "      (qvk): Linear(in_features=400, out_features=1200, bias=True)\n",
              "    )\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (causal_attention): MultiheadAttention(\n",
              "      (o): Linear(in_features=400, out_features=400, bias=True)\n",
              "    )\n",
              "    (feedforward): Sequential(\n",
              "      (0): Linear(in_features=400, out_features=1024, bias=True)\n",
              "      (1): Dropout(p=0.2, inplace=False)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): Linear(in_features=1024, out_features=400, bias=True)\n",
              "    )\n",
              "    (norm1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (output_layer): Sequential(\n",
              "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
              "    (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.4, inplace=False)\n",
              "    (4): Linear(in_features=400, out_features=33243, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for rep in range(10):\n",
        "  epochs = 10             \n",
        "  lr = 5e-4               \n",
        "  clip = 0.5\n",
        "  bptt = 512\n",
        "  # best_val_loss = float('inf')\n",
        "  # best_model = None\n",
        "  steps_per_epoch = train_data.size(1)//bptt+1\n",
        "  total_steps = epochs*(steps_per_epoch)\n",
        "  opt_args = {\n",
        "      'lr': 0,\n",
        "      'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "  }\n",
        "\n",
        "  linear_args = {\n",
        "      'total_steps': total_steps,\n",
        "      'pct_start': 0.1, 'anneal_strategy': 'linear',\n",
        "      'three_phase': True, 'max_lr': 1e-3\n",
        "  }\n",
        "  opt = torch.optim.RAdam(model.parameters(),\n",
        "                          **opt_args)\n",
        "  schedular_args = linear_args\n",
        "  schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)\n",
        "  optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      epoch_start_time = time.time()\n",
        "      train(model)\n",
        "      val_loss = evaluate(model, val_data)\n",
        "      val_ppl = math.exp(val_loss)\n",
        "      elapsed = time.time() - epoch_start_time\n",
        "      print('-' * 89)\n",
        "      print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "      print('-' * 89)\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          del best_model\n",
        "          best_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "vxsIgBSvLhrY",
        "outputId": "bd92200d-6037-429b-a6de-bd9ecc54f9e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  217 batches | lr 0.000938 | ms/batch 128.12 | loss  3.87 | ppl    48.07| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.87s | valid loss  4.03 | valid ppl    56.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000070 | ms/batch 129.22 | loss  3.97 | ppl    52.96| seq_len   506\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.86s | valid loss  3.96 | valid ppl    52.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000036 | ms/batch 130.71 | loss  3.80 | ppl    44.60| seq_len   519\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.99s | valid loss  3.95 | valid ppl    51.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000015 | ms/batch 128.73 | loss  3.78 | ppl    43.91| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.90s | valid loss  3.94 | valid ppl    51.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 130.76 | loss  3.77 | ppl    43.32| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 30.00s | valid loss  3.94 | valid ppl    51.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000019 | ms/batch 129.35 | loss  3.76 | ppl    42.80| seq_len   497\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.97s | valid loss  3.94 | valid ppl    51.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000015 | ms/batch 129.25 | loss  3.75 | ppl    42.58| seq_len   519\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.93s | valid loss  3.94 | valid ppl    51.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000009 | ms/batch 131.06 | loss  3.75 | ppl    42.32| seq_len   511\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 30.02s | valid loss  3.94 | valid ppl    51.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 128.82 | loss  3.74 | ppl    42.00| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.92s | valid loss  3.95 | valid ppl    51.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 130.54 | loss  3.74 | ppl    41.93| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 30.00s | valid loss  3.95 | valid ppl    51.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000924 | ms/batch 129.30 | loss  3.87 | ppl    48.14| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.97s | valid loss  4.03 | valid ppl    56.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000076 | ms/batch 130.20 | loss  3.97 | ppl    52.76| seq_len   514\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 30.00s | valid loss  3.96 | valid ppl    52.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 129.44 | loss  3.80 | ppl    44.54| seq_len   512\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.96s | valid loss  3.95 | valid ppl    51.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 129.84 | loss  3.77 | ppl    43.57| seq_len   514\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.94s | valid loss  3.94 | valid ppl    51.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000024 | ms/batch 131.63 | loss  3.76 | ppl    43.00| seq_len   504\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 30.03s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000010 | ms/batch 128.37 | loss  3.76 | ppl    42.85| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.93s | valid loss  3.94 | valid ppl    51.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000014 | ms/batch 132.19 | loss  3.75 | ppl    42.47| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 30.00s | valid loss  3.94 | valid ppl    51.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000010 | ms/batch 129.00 | loss  3.74 | ppl    42.15| seq_len   523\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.97s | valid loss  3.94 | valid ppl    51.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 131.03 | loss  3.73 | ppl    41.80| seq_len   515\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 30.02s | valid loss  3.94 | valid ppl    51.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 131.33 | loss  3.73 | ppl    41.59| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.98s | valid loss  3.95 | valid ppl    51.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000920 | ms/batch 130.11 | loss  3.87 | ppl    48.05| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.89s | valid loss  4.04 | valid ppl    56.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000084 | ms/batch 129.86 | loss  3.97 | ppl    52.73| seq_len   513\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.95s | valid loss  3.96 | valid ppl    52.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 127.99 | loss  3.79 | ppl    44.45| seq_len   511\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.92s | valid loss  3.95 | valid ppl    51.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000015 | ms/batch 130.18 | loss  3.77 | ppl    43.60| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 30.00s | valid loss  3.94 | valid ppl    51.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 131.66 | loss  3.76 | ppl    43.03| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 30.02s | valid loss  3.94 | valid ppl    51.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 128.00 | loss  3.75 | ppl    42.59| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.95s | valid loss  3.94 | valid ppl    51.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000014 | ms/batch 130.96 | loss  3.74 | ppl    42.10| seq_len   499\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.99s | valid loss  3.94 | valid ppl    51.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000010 | ms/batch 131.52 | loss  3.74 | ppl    42.19| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 30.05s | valid loss  3.94 | valid ppl    51.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 127.42 | loss  3.74 | ppl    41.91| seq_len   520\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.90s | valid loss  3.94 | valid ppl    51.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 129.41 | loss  3.73 | ppl    41.70| seq_len   499\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.87s | valid loss  3.95 | valid ppl    51.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000909 | ms/batch 130.40 | loss  3.87 | ppl    47.78| seq_len   501\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 30.00s | valid loss  4.04 | valid ppl    56.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000089 | ms/batch 130.82 | loss  3.96 | ppl    52.71| seq_len   512\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.97s | valid loss  3.96 | valid ppl    52.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 128.81 | loss  3.79 | ppl    44.33| seq_len   514\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.96s | valid loss  3.95 | valid ppl    51.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 130.51 | loss  3.77 | ppl    43.50| seq_len   511\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.98s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 130.78 | loss  3.76 | ppl    42.89| seq_len   520\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 30.03s | valid loss  3.94 | valid ppl    51.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 131.87 | loss  3.75 | ppl    42.45| seq_len   524\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 30.07s | valid loss  3.94 | valid ppl    51.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000015 | ms/batch 132.08 | loss  3.74 | ppl    42.25| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 30.09s | valid loss  3.94 | valid ppl    51.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000010 | ms/batch 129.74 | loss  3.73 | ppl    41.64| seq_len   516\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.98s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000005 | ms/batch 129.35 | loss  3.73 | ppl    41.59| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.97s | valid loss  3.94 | valid ppl    51.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 129.57 | loss  3.72 | ppl    41.41| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.96s | valid loss  3.94 | valid ppl    51.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000917 | ms/batch 130.50 | loss  3.86 | ppl    47.63| seq_len   505\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.96s | valid loss  4.04 | valid ppl    56.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000083 | ms/batch 129.26 | loss  3.96 | ppl    52.54| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.94s | valid loss  3.96 | valid ppl    52.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 129.78 | loss  3.79 | ppl    44.06| seq_len   517\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.98s | valid loss  3.95 | valid ppl    51.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 129.43 | loss  3.77 | ppl    43.38| seq_len   510\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.93s | valid loss  3.94 | valid ppl    51.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 131.40 | loss  3.76 | ppl    42.85| seq_len   515\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 30.07s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 130.87 | loss  3.74 | ppl    42.29| seq_len   516\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 30.00s | valid loss  3.94 | valid ppl    51.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000014 | ms/batch 130.12 | loss  3.74 | ppl    42.01| seq_len   505\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.97s | valid loss  3.94 | valid ppl    51.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000009 | ms/batch 128.22 | loss  3.73 | ppl    41.69| seq_len   512\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.93s | valid loss  3.94 | valid ppl    51.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 131.58 | loss  3.73 | ppl    41.75| seq_len   511\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 30.06s | valid loss  3.94 | valid ppl    51.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 129.35 | loss  3.73 | ppl    41.53| seq_len   516\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.98s | valid loss  3.95 | valid ppl    51.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000913 | ms/batch 130.22 | loss  3.86 | ppl    47.64| seq_len   503\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.98s | valid loss  4.04 | valid ppl    56.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000089 | ms/batch 129.57 | loss  3.97 | ppl    52.88| seq_len   516\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.98s | valid loss  3.96 | valid ppl    52.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 131.14 | loss  3.78 | ppl    43.98| seq_len   506\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 30.00s | valid loss  3.95 | valid ppl    51.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 131.23 | loss  3.77 | ppl    43.21| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 30.02s | valid loss  3.94 | valid ppl    51.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 130.01 | loss  3.75 | ppl    42.59| seq_len   512\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 29.96s | valid loss  3.94 | valid ppl    51.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 131.24 | loss  3.74 | ppl    42.16| seq_len   513\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.87s | valid loss  3.94 | valid ppl    51.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000015 | ms/batch 128.53 | loss  3.73 | ppl    41.82| seq_len   514\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.95s | valid loss  3.94 | valid ppl    51.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000009 | ms/batch 130.84 | loss  3.73 | ppl    41.57| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.99s | valid loss  3.94 | valid ppl    51.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 131.85 | loss  3.72 | ppl    41.29| seq_len   512\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 30.02s | valid loss  3.94 | valid ppl    51.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 127.71 | loss  3.72 | ppl    41.08| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.87s | valid loss  3.95 | valid ppl    51.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000940 | ms/batch 129.90 | loss  3.86 | ppl    47.42| seq_len   518\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.88s | valid loss  4.03 | valid ppl    56.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000084 | ms/batch 130.39 | loss  3.96 | ppl    52.40| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.94s | valid loss  3.96 | valid ppl    52.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 127.46 | loss  3.78 | ppl    43.77| seq_len   504\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.93s | valid loss  3.95 | valid ppl    51.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 131.40 | loss  3.76 | ppl    43.14| seq_len   518\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 30.10s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 130.11 | loss  3.75 | ppl    42.52| seq_len   511\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 29.92s | valid loss  3.94 | valid ppl    51.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 129.62 | loss  3.74 | ppl    42.08| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.97s | valid loss  3.94 | valid ppl    51.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000014 | ms/batch 131.04 | loss  3.73 | ppl    41.64| seq_len   505\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 30.00s | valid loss  3.94 | valid ppl    51.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000010 | ms/batch 131.69 | loss  3.72 | ppl    41.46| seq_len   515\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 30.04s | valid loss  3.94 | valid ppl    51.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 130.49 | loss  3.72 | ppl    41.38| seq_len   513\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 30.00s | valid loss  3.94 | valid ppl    51.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 130.61 | loss  3.71 | ppl    41.01| seq_len   513\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 29.99s | valid loss  3.94 | valid ppl    51.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000922 | ms/batch 130.91 | loss  3.86 | ppl    47.43| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 30.01s | valid loss  4.03 | valid ppl    55.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000089 | ms/batch 129.98 | loss  3.96 | ppl    52.40| seq_len   515\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.99s | valid loss  3.96 | valid ppl    52.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000035 | ms/batch 129.83 | loss  3.78 | ppl    43.79| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.96s | valid loss  3.95 | valid ppl    51.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000030 | ms/batch 127.93 | loss  3.77 | ppl    43.33| seq_len   507\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.99s | valid loss  3.94 | valid ppl    51.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 129.62 | loss  3.75 | ppl    42.54| seq_len   506\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 29.93s | valid loss  3.94 | valid ppl    51.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000019 | ms/batch 131.72 | loss  3.74 | ppl    41.99| seq_len   506\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.97s | valid loss  3.94 | valid ppl    51.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000015 | ms/batch 131.28 | loss  3.73 | ppl    41.61| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.96s | valid loss  3.94 | valid ppl    51.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000010 | ms/batch 130.07 | loss  3.72 | ppl    41.39| seq_len   510\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.95s | valid loss  3.94 | valid ppl    51.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 128.63 | loss  3.72 | ppl    41.17| seq_len   513\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.90s | valid loss  3.94 | valid ppl    51.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 129.77 | loss  3.71 | ppl    41.02| seq_len   520\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 30.02s | valid loss  3.94 | valid ppl    51.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000933 | ms/batch 130.48 | loss  3.86 | ppl    47.25| seq_len   514\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 29.97s | valid loss  4.03 | valid ppl    56.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  217 batches | lr 0.000085 | ms/batch 129.85 | loss  3.95 | ppl    52.09| seq_len   518\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 29.99s | valid loss  3.96 | valid ppl    52.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  217 batches | lr 0.000036 | ms/batch 128.82 | loss  3.78 | ppl    43.77| seq_len   519\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 29.92s | valid loss  3.95 | valid ppl    51.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  217 batches | lr 0.000015 | ms/batch 128.70 | loss  3.75 | ppl    42.69| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 29.92s | valid loss  3.94 | valid ppl    51.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  217 batches | lr 0.000025 | ms/batch 128.88 | loss  3.75 | ppl    42.40| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 29.95s | valid loss  3.94 | valid ppl    51.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  217 batches | lr 0.000020 | ms/batch 128.87 | loss  3.73 | ppl    41.67| seq_len   509\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 29.91s | valid loss  3.94 | valid ppl    51.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  217 batches | lr 0.000014 | ms/batch 129.03 | loss  3.73 | ppl    41.49| seq_len   508\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 29.96s | valid loss  3.94 | valid ppl    51.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  217 batches | lr 0.000009 | ms/batch 127.84 | loss  3.72 | ppl    41.11| seq_len   503\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 29.84s | valid loss  3.94 | valid ppl    51.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  217 batches | lr 0.000004 | ms/batch 129.46 | loss  3.71 | ppl    40.94| seq_len   518\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 29.98s | valid loss  3.94 | valid ppl    51.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  217 batches | lr 0.000001 | ms/batch 131.00 | loss  3.71 | ppl    40.87| seq_len   510\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 30.05s | valid loss  3.94 | valid ppl    51.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   1 |   200/  217 batches | lr 0.000935 | ms/batch 131.03 | loss  3.86 | ppl    47.31| seq_len   515\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 30.06s | valid loss  4.03 | valid ppl    56.01\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-fee2bc4f9b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-5acd41d6dab6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not important"
      ],
      "metadata": {
        "id": "ON8LmbvYdMGU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndJCTLgBuQHp"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  num_classes = train_loader.dataset.num_categories\n",
        "  l = 0\n",
        "  acc = 0\n",
        "  pbar = tqdm(total = len(train_loader),position=0,leave=True)\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    x, output_shifted = data\n",
        "    preds = model(x.to(device), output_shifted.to(device))       \n",
        "    loss = criterion(preds.view(-1,preds.size(-1)), target.view(-1),reduction=\"mean\")\n",
        "    loss.backward() \n",
        "    #nn.utils.clip_grad_norm(model.parameters(), clip)       \n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      current_loss = loss.item()\n",
        "      l+= loss.item()\n",
        "      acc+=(preds.argmax(dim=-1) == target).float().mean().item()\n",
        "    pbar.set_description('training_step {} loss:{:3f}'.format(batch_idx,current_loss))\n",
        "    pbar.update()\n",
        "  acc = 100. * acc / (len(train_loader))\n",
        "  l = l/len(train_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format('train',l,acc))\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOzqXY-WwNlr"
      },
      "source": [
        "def test( model, device, test_loader,criterion,mode='eval'):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  num_classes=test_loader.dataset.num_categories\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          target = target.to(device)\n",
        "          x, output_shifted = data\n",
        "          output = model(x.to(device), output_shifted.to(device))\n",
        "          test_loss += criterion(output.view(-1,output.size(-1)),\\\n",
        "                                        target.view(-1)).item()          \n",
        "          correct += (output.argmax(dim=-1) == target).float().mean().item()\n",
        "\n",
        "  loss = test_loss/len(test_loader)\n",
        "  acc = 100. * correct / len(test_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format(mode,loss,acc))\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raohC_7iRk0N"
      },
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(low=1, high=self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        labels_shifted = labels.roll(1,0)\n",
        "        labels_shifted[0] = torch.tensor(0)\n",
        "        return (inp_data,labels_shifted), labels"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8jEEssRQX5"
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.determinstic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dataset = partial(ReverseDataset, 10, 16)\n",
        "    train_loader = data.DataLoader(dataset(50000), batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    \n",
        "    val_loader   = data.DataLoader(dataset(1000), batch_size=test_batch_size)\n",
        "    test_loader  = data.DataLoader(dataset(10000), batch_size=test_batch_size)\n",
        "    model = UnetTransformer(n_blocks=n_blocks,input_dim=train_loader.dataset.num_categories,\\\n",
        "                            n_heads=n_heads,d_model = h_dims,num_classes=\\\n",
        "                            train_loader.dataset.num_categories,dim_feedforward = h_dims,\\\n",
        "                            dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "    criterion =  F.cross_entropy\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "       train(model, device, train_loader, optimizer,criterion)\n",
        "       test(model, device, val_loader,criterion)\n",
        "        \n",
        "    torch.save(model.state_dict(), \"model.h5\")\n",
        "    print('------------testing--------------')\n",
        "    test(model, device, test_loader,criterion,mode='test')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #main()"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}