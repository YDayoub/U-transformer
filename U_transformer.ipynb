{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgaZDRUnRQAUv+I5YbECev",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDayoub/U-transformer/blob/main/U_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsHozpzlIsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05539bce-9209-4264-8dce-19e03329f1a3"
      },
      "source": [
        "'''\n",
        "Import required libraries\n",
        "'''\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Tuple\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrbb2GkSKiJ"
      },
      "source": [
        "# batch_size = 128          \n",
        "# test_batch_size = 128   \n",
        "epochs = 100             \n",
        "lr = 5e-4               \n",
        "seed = 42               \n",
        "h_dims = 1024\n",
        "n_heads = 16\n",
        "n_blocks = 2\n",
        "dropout = 0.2\n",
        "clip = 0.5\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "bptt = 256\n",
        "d_model = 400"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvxLamYx-zF"
      },
      "source": [
        "'''\n",
        "This code is adapted from \n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "'''\n",
        "class Embedding_with_PosEncoding(nn.Module):\n",
        "  def __init__(self,input_dim,d_model, max_len=5000,dropout=0):\n",
        "    '''\n",
        "    Args:\n",
        "      d_model: hidden space dimentionality for Embedding\n",
        "      input_dim: input space dimentionality\n",
        "      max_len: maximum length of an input sequence\n",
        "      drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(Embedding_with_PosEncoding,self).__init__()\n",
        "    self.emb = nn.Embedding(input_dim,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    # register_buffer is used to save and retrain parameters which don't need to train\n",
        "    self.register_buffer('pe', pe, persistent=False) \n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    x = self.emb(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + self.pe[:, :seq_len]\n",
        "    return x\n",
        "  def get_pe(self):\n",
        "    return self.pe"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2k37K8dSjZB"
      },
      "source": [
        "def test_positional_encoding():\n",
        "  batch_dim,seq_len,input_dim= (15,10,10)\n",
        "  d_model = 100\n",
        "  max_len =100\n",
        "  x = torch.randint(low=0, high=10,size=(batch_dim,seq_len))\n",
        "  pos_encoder = Embedding_with_PosEncoding(input_dim,d_model,max_len)\n",
        "  pe = pos_encoder.get_pe()\n",
        "  res = pos_encoder(x)\n",
        "  assert res.shape ==  torch.Size([batch_dim,seq_len,d_model])\n",
        "  assert pe.shape == torch.Size([1, max_len, d_model])\n",
        "test_positional_encoding()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NzV-cFUbdz"
      },
      "source": [
        "def scaled_dot_product(query,key,values,mask=None,scale=True):\n",
        "  '''\n",
        "      Args:\n",
        "        query: tensor of queries\n",
        "        key : tensor of keys\n",
        "        value: tensor of value\n",
        "        mask (numpy.ndarray): attention-mask, used to perform self attention when required\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "  '''\n",
        "  if scale:\n",
        "    depth = query.shape[-1] ** 0.5\n",
        "  else:\n",
        "    depth = 1\n",
        "  dots = torch.matmul(query,torch.swapaxes(key,-1,-2))/depth\n",
        "  if mask is not None:\n",
        "    dots = torch.where(mask,dots,torch.full_like(dots, -9e15))\n",
        "  logsumexp = torch.logsumexp(dots, axis=-1, keepdims=True)\n",
        "  dots = torch.exp(dots - logsumexp)\n",
        "  attention = torch.matmul(dots, values)\n",
        "  return attention\n",
        "def dot_product_self_attention(q, k, v,device=device):\n",
        "  '''\n",
        "    Args:\n",
        "        q: queries.\n",
        "        k: keys.\n",
        "        v: values.\n",
        "    Returns:\n",
        "        masked dot product self attention tensor.  \n",
        "  '''\n",
        "  mask_size = q.shape[-2]\n",
        "  mask = torch.tril(torch.ones((1, mask_size, mask_size), dtype=torch.bool), diagonal=0).to(device)        \n",
        "  return scaled_dot_product(q, k, v, mask)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA0CRdjiU8H"
      },
      "source": [
        "class QKV(nn.Module):\n",
        "  '''\n",
        "  takes as input a tensor of shape (batch_size,seq_len,d_model)\n",
        "  returns:\n",
        "  three tensors q,k,v of shape (batch_size,n_heads,seq_len,d_model//n_heads)\n",
        "  '''\n",
        "\n",
        "  def __init__(self,n_heads,d_model):\n",
        "    '''\n",
        "      Args:\n",
        "        n_heads: number of heads used in multihead attention\n",
        "        d_model: hidden space dimensions\n",
        "    '''\n",
        "    assert d_model%n_heads==0,'d_models should be divisible by n_heads'\n",
        "    super(QKV,self).__init__()\n",
        "    self.qvk = nn.Linear(in_features=d_model,out_features=3*d_model)\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_heads = d_model//n_heads\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len,d_model = x.shape\n",
        "    x = self.qvk(x)\n",
        "    x = x.reshape(batch_size,seq_len,self.n_heads,3*self.d_heads)\n",
        "    x = x.permute(0,2,1,3)\n",
        "    q,k,v = x.chunk(3,dim=-1)\n",
        "    return q,k,v\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JEfLDB7oW_"
      },
      "source": [
        "def test_QKV():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  x = torch.randn(batch_dim,seq_len, d_model).to(device)\n",
        "  qkv = QKV(n_heads=n_heads,d_model=d_model).to(device)\n",
        "  q,k,v = qkv(x)\n",
        "  assert q.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert k.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert v.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "test_QKV()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CR3TtPk2wnr"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  '''\n",
        "  This class implements mulithead attention\n",
        "  '''\n",
        "  def __init__(self,d_model,causal_attention=False):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        causal_attention: boolean whether to use attention or causal attention \n",
        "    '''\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.o = nn.Linear(in_features=d_model,out_features=d_model)\n",
        "    self.causal_attention = causal_attention \n",
        "\n",
        "  def forward(self,q,k,v):\n",
        "    batch_size,n_heads,seq_len,d_heads = q.shape\n",
        "    if self.causal_attention:\n",
        "      atten =  dot_product_self_attention(q, k, v)\n",
        "    else:\n",
        "      atten = scaled_dot_product(q,k,v)\n",
        "    atten = atten.permute(0,2,1,3)\n",
        "    atten = atten.reshape(batch_size,seq_len,self.d_model)\n",
        "    res = self.o(atten)\n",
        "    return res\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTh48ZAH8mSw"
      },
      "source": [
        "def test_MultiheadAttention():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  att = MultiheadAttention(d_model,causal_attention=False).to(device)\n",
        "  causal_att = MultiheadAttention(d_model,causal_attention=True).to(device)\n",
        "  x = torch.randn(batch_dim, n_heads, seq_len,3,d_model//n_heads).to(device)\n",
        "  q,k,v = x[:,:,:,0,:],x[:,:,:,1,:],x[:,:,:,2,:]\n",
        "  o1 = att(q,k,v)\n",
        "  o2 = causal_att(q,k,v)\n",
        "  assert o1.shape ==  torch.Size([batch_dim, seq_len,d_model])\n",
        "  assert o2.shape ==  torch.Size([batch_dim,  seq_len,d_model])\n",
        "test_MultiheadAttention()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTC9-skP2ory"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  '''\n",
        "  This class implements encoder block\n",
        "  '''\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(EncoderBlock,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.qkv =  QKV(n_heads=n_heads,d_model=d_model)\n",
        "    self.attention = MultiheadAttention(d_model=d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = self.feedforward(x2)\n",
        "    x4 = self.norm2(self.dropout(x3)+x2)\n",
        "    return x4\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWmFwuHFO46"
      },
      "source": [
        "def reshape_tensor(x,n_heads):\n",
        "  '''\n",
        "    Args:\n",
        "      x: tensor of shape (batch_size,seq_len,d_model)\n",
        "      n_heads: number of heads in mutlihead attention\n",
        "    Returns:\n",
        "      reshaped tensor of shape (batch_size,n_heads,seq_len,d_model//n_heads)    \n",
        "  '''\n",
        "  batch_size,seq_len,d_model = x.shape\n",
        "  x = x.reshape(batch_size,seq_len,n_heads,d_model//n_heads)\n",
        "  x = x.permute(0,2,1,3)\n",
        "  return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  '''\n",
        "    This class implements decoder block\n",
        "  '''\n",
        "\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.qkv = QKV(n_heads,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.causal_attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0,skip_con):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.causal_attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = reshape_tensor(x2,self.n_heads)\n",
        "    skip_con = reshape_tensor(skip_con,self.n_heads)\n",
        "    x4 = self.attention(x3,skip_con,skip_con)\n",
        "    x5 = self.norm2(x2+self.dropout(x4))\n",
        "    x6 = self.feedforward(x5)\n",
        "    x7 = self.norm3(self.dropout(x6)+x5)\n",
        "    return x7"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG1NMhDKAvw"
      },
      "source": [
        "class UnetTransformer(nn.Module):\n",
        "  '''\n",
        "    This class implements unet transformer\n",
        "  '''\n",
        "  def __init__(self,n_blocks,n_tokens,n_heads,d_model,dim_feedforward,emb_dropout=0.2,\\\n",
        "               out_dropout = 0.4, dropout=0.2):\n",
        "\n",
        "    '''\n",
        "      Args:\n",
        "        n_blocks: number of encoder/decoder blocks\n",
        "        n_tokens: Dimensionality of the input space\n",
        "        n_heads: number of heads in MultiHeadAttention\n",
        "        d_model: Dimensionality of the embedding space\n",
        "        num_classes: Dimensionality of the output space\n",
        "        dim_feedforward:  Dimensionality of the hidden layer in the MLP \n",
        "\n",
        "\n",
        "    '''\n",
        "    super(UnetTransformer,self).__init__()\n",
        "    self.n_blocks = n_blocks\n",
        "    self.pos_enc = Embedding_with_PosEncoding(n_tokens,d_model,dropout=emb_dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['enc_'+str(i)] = EncoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['dec_'+str(i)] = DecoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    self.output_layer = nn.Sequential( nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(out_dropout),\n",
        "            nn.Linear(d_model, n_tokens)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "    x_encoded = self.pos_enc(x)\n",
        "    layers = vars(self)['_modules']\n",
        "    stack = [x_encoded]\n",
        "    x = layers['enc_0'](x_encoded)\n",
        "    for i in range(1,self.n_blocks):\n",
        "      stack.append(x)\n",
        "      x = layers['enc_'+str(i)](x)\n",
        "    stack.append(x)\n",
        "    x = layers['dec_0'](x,stack.pop(0))\n",
        "    for i in range(1,self.n_blocks):\n",
        "      x = layers['dec_'+str(i)](x,stack.pop(0))\n",
        "    return self.output_layer(x)\n",
        "\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset"
      ],
      "metadata": {
        "id": "BcWK--CdaBXU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "c2n815-3aGA4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, source.shape[1] - 1 - i)\n",
        "    data = source[:,i:i+seq_len]\n",
        "    target = source[:,i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "get_batch(train_data,0)[0].shape"
      ],
      "metadata": {
        "id": "0CGLbgM1aS2m",
        "outputId": "1d52ea3d-e914-49f7-d4c1-17655e92bcb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "print('n_tokens {}'.format(len(vocab)))\n",
        "model = UnetTransformer(n_blocks=n_blocks,n_tokens=ntokens,\\\n",
        "                        n_heads=n_heads, d_model = d_model,dim_feedforward = h_dims,\\\n",
        "                        dropout=dropout).to(device)\n",
        "pytorch_total_params = sum(p.numel()\n",
        "                        for p in model.parameters() if p.requires_grad)\n",
        "print('-' * 89)\n",
        "print(\n",
        "    '#'*12+f\" Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters for {epochs:3d} epochs \"+'#'*12)\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8us_WwAIaup5",
        "outputId": "5b65b630-0c31-45ee-bde8-8bfe2bc57fdf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens 33243\n",
            "-----------------------------------------------------------------------------------------\n",
            "############ Training model with 32.97M trainable parameters for 100 epochs ############\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicOpt:\n",
        "    def __init__(self, optimizer, schedular):\n",
        "        self.optimizer = optimizer\n",
        "        self.schedular = schedular\n",
        "        self._scalar = 1\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    @property\n",
        "    def lr(self):\n",
        "      return self.optimizer.param_groups[0]['lr']*self._scalar\n",
        "\n",
        "    @property\n",
        "    def scalar(self):\n",
        "        return self._scalar\n",
        "\n",
        "    @scalar.setter\n",
        "    def scalar(self, scalar):\n",
        "        self._scalar = scalar\n",
        "\n",
        "\n",
        "    def schedule_step(self, val_loss):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class linearcycleWarmup(BasicOpt):\n",
        "    def __init__(self, optimizer, schedular, *args, **kwargs):\n",
        "        super().__init__(optimizer=optimizer, schedular=schedular)\n",
        "        self.use_scheduler = True\n",
        "\n",
        "       \n",
        "    def step(self):\n",
        "        lr_s = [p['lr'] for p in self.optimizer.param_groups]\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr']  = p['lr']*self._scalar             \n",
        "        self.optimizer.step()\n",
        "        for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = lr_s[idx]\n",
        "        try:\n",
        "          if self.use_scheduler:\n",
        "            self.schedular.step()\n",
        "        except Exception as e:\n",
        "          self.use_scheduler = False\n",
        "          for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = 0.00000088\n",
        "\n",
        "\n",
        "\n",
        "    def schedule_step(self, *args):\n",
        "        pass\n",
        "steps_per_epoch = train_data.size(1)//bptt+1\n",
        "total_steps = epochs*(steps_per_epoch)\n",
        "opt_args = {\n",
        "    'lr': 0,\n",
        "    'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "}\n",
        "\n",
        "linear_args = {\n",
        "    'total_steps': total_steps,\n",
        "    'pct_start': 0.3, 'anneal_strategy': 'linear',\n",
        "    'three_phase': True, 'max_lr': 1e-3\n",
        "}\n",
        "opt = torch.optim.RAdam(model.parameters(),\n",
        "                        **opt_args)\n",
        "schedular_args = linear_args\n",
        "schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)"
      ],
      "metadata": {
        "id": "yk9NtX6gf9qw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = train_data.shape[1] // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(1) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(1)\n",
        "        # if batch_size != bptt:  # only on last batch\n",
        "        #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = optimizer.lr\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(1) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(1)\n",
        "            # if batch_size != bptt:\n",
        "            #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (eval_data.size(1) - 1)"
      ],
      "metadata": {
        "id": "Gspv87NsaWha"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHvZbtTJaa-4",
        "outputId": "5d7efb45-b807-42ee-e57e-5687f0fca71c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  434 batches | lr 0.000055 | ms/batch 112.42 | loss  9.33 | ppl 11238.88\n",
            "| epoch   1 |   400/  434 batches | lr 0.000070 | ms/batch 111.85 | loss  7.25 | ppl  1403.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 50.39s | valid loss  5.95 | valid ppl   384.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  434 batches | lr 0.000087 | ms/batch 112.12 | loss  6.40 | ppl   602.20\n",
            "| epoch   2 |   400/  434 batches | lr 0.000102 | ms/batch 111.83 | loss  6.18 | ppl   484.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 50.26s | valid loss  5.51 | valid ppl   247.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  434 batches | lr 0.000119 | ms/batch 112.54 | loss  6.02 | ppl   409.84\n",
            "| epoch   3 |   400/  434 batches | lr 0.000134 | ms/batch 112.42 | loss  5.86 | ppl   352.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 50.47s | valid loss  5.28 | valid ppl   197.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  434 batches | lr 0.000151 | ms/batch 112.84 | loss  5.77 | ppl   319.22\n",
            "| epoch   4 |   400/  434 batches | lr 0.000166 | ms/batch 112.86 | loss  5.65 | ppl   284.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 50.61s | valid loss  5.14 | valid ppl   171.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  434 batches | lr 0.000183 | ms/batch 112.58 | loss  5.58 | ppl   265.86\n",
            "| epoch   5 |   400/  434 batches | lr 0.000198 | ms/batch 111.92 | loss  5.49 | ppl   241.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 50.37s | valid loss  5.03 | valid ppl   152.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  434 batches | lr 0.000215 | ms/batch 112.86 | loss  5.43 | ppl   228.22\n",
            "| epoch   6 |   400/  434 batches | lr 0.000230 | ms/batch 111.64 | loss  5.35 | ppl   210.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 50.38s | valid loss  4.95 | valid ppl   141.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  434 batches | lr 0.000247 | ms/batch 112.32 | loss  5.31 | ppl   201.59\n",
            "| epoch   7 |   400/  434 batches | lr 0.000262 | ms/batch 112.23 | loss  5.23 | ppl   187.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 50.40s | valid loss  4.88 | valid ppl   131.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  434 batches | lr 0.000279 | ms/batch 112.18 | loss  5.20 | ppl   180.83\n",
            "| epoch   8 |   400/  434 batches | lr 0.000294 | ms/batch 111.41 | loss  5.13 | ppl   169.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 50.15s | valid loss  4.83 | valid ppl   124.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  434 batches | lr 0.000311 | ms/batch 112.82 | loss  5.10 | ppl   164.17\n",
            "| epoch   9 |   400/  434 batches | lr 0.000326 | ms/batch 112.39 | loss  5.04 | ppl   155.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 50.55s | valid loss  4.78 | valid ppl   118.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  434 batches | lr 0.000343 | ms/batch 112.27 | loss  5.02 | ppl   150.97\n",
            "| epoch  10 |   400/  434 batches | lr 0.000358 | ms/batch 111.76 | loss  4.97 | ppl   143.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 50.30s | valid loss  4.74 | valid ppl   114.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/  434 batches | lr 0.000375 | ms/batch 113.29 | loss  4.95 | ppl   140.85\n",
            "| epoch  11 |   400/  434 batches | lr 0.000390 | ms/batch 111.71 | loss  4.90 | ppl   134.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 50.49s | valid loss  4.71 | valid ppl   111.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/  434 batches | lr 0.000407 | ms/batch 112.76 | loss  4.89 | ppl   132.67\n",
            "| epoch  12 |   400/  434 batches | lr 0.000422 | ms/batch 111.76 | loss  4.85 | ppl   127.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 50.35s | valid loss  4.67 | valid ppl   107.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/  434 batches | lr 0.000439 | ms/batch 112.53 | loss  4.84 | ppl   126.03\n",
            "| epoch  13 |   400/  434 batches | lr 0.000454 | ms/batch 111.71 | loss  4.81 | ppl   122.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 50.35s | valid loss  4.64 | valid ppl   103.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/  434 batches | lr 0.000471 | ms/batch 112.45 | loss  4.80 | ppl   121.24\n",
            "| epoch  14 |   400/  434 batches | lr 0.000486 | ms/batch 112.06 | loss  4.77 | ppl   117.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 50.36s | valid loss  4.61 | valid ppl   100.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/  434 batches | lr 0.000503 | ms/batch 113.11 | loss  4.76 | ppl   117.14\n",
            "| epoch  15 |   400/  434 batches | lr 0.000518 | ms/batch 112.28 | loss  4.73 | ppl   113.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 50.58s | valid loss  4.58 | valid ppl    97.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/  434 batches | lr 0.000535 | ms/batch 112.83 | loss  4.73 | ppl   113.57\n",
            "| epoch  16 |   400/  434 batches | lr 0.000550 | ms/batch 111.50 | loss  4.71 | ppl   110.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 50.36s | valid loss  4.55 | valid ppl    94.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/  434 batches | lr 0.000567 | ms/batch 113.38 | loss  4.70 | ppl   110.32\n",
            "| epoch  17 |   400/  434 batches | lr 0.000582 | ms/batch 112.60 | loss  4.68 | ppl   107.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 50.72s | valid loss  4.54 | valid ppl    93.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/  434 batches | lr 0.000599 | ms/batch 113.29 | loss  4.68 | ppl   107.28\n",
            "| epoch  18 |   400/  434 batches | lr 0.000614 | ms/batch 112.30 | loss  4.65 | ppl   104.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 50.61s | valid loss  4.52 | valid ppl    91.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/  434 batches | lr 0.000631 | ms/batch 112.11 | loss  4.65 | ppl   104.40\n",
            "| epoch  19 |   400/  434 batches | lr 0.000646 | ms/batch 111.99 | loss  4.63 | ppl   102.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 50.29s | valid loss  4.50 | valid ppl    89.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/  434 batches | lr 0.000663 | ms/batch 112.80 | loss  4.62 | ppl   101.59\n",
            "| epoch  20 |   400/  434 batches | lr 0.000678 | ms/batch 112.73 | loss  4.60 | ppl    99.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 50.61s | valid loss  4.47 | valid ppl    87.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/  434 batches | lr 0.000695 | ms/batch 113.16 | loss  4.59 | ppl    98.85\n",
            "| epoch  21 |   400/  434 batches | lr 0.000710 | ms/batch 112.31 | loss  4.58 | ppl    97.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 50.57s | valid loss  4.46 | valid ppl    86.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/  434 batches | lr 0.000727 | ms/batch 113.45 | loss  4.57 | ppl    96.46\n",
            "| epoch  22 |   400/  434 batches | lr 0.000742 | ms/batch 112.48 | loss  4.55 | ppl    95.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 50.68s | valid loss  4.43 | valid ppl    84.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/  434 batches | lr 0.000759 | ms/batch 112.66 | loss  4.55 | ppl    94.63\n",
            "| epoch  23 |   400/  434 batches | lr 0.000774 | ms/batch 112.51 | loss  4.53 | ppl    93.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 50.57s | valid loss  4.41 | valid ppl    82.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/  434 batches | lr 0.000791 | ms/batch 113.14 | loss  4.53 | ppl    92.60\n",
            "| epoch  24 |   400/  434 batches | lr 0.000806 | ms/batch 112.41 | loss  4.51 | ppl    91.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 50.60s | valid loss  4.39 | valid ppl    80.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/  434 batches | lr 0.000823 | ms/batch 113.46 | loss  4.51 | ppl    90.70\n",
            "| epoch  25 |   400/  434 batches | lr 0.000838 | ms/batch 112.64 | loss  4.49 | ppl    89.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 50.70s | valid loss  4.39 | valid ppl    80.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/  434 batches | lr 0.000855 | ms/batch 113.81 | loss  4.49 | ppl    89.08\n",
            "| epoch  26 |   400/  434 batches | lr 0.000870 | ms/batch 112.32 | loss  4.48 | ppl    88.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 50.73s | valid loss  4.37 | valid ppl    79.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/  434 batches | lr 0.000887 | ms/batch 112.92 | loss  4.47 | ppl    87.43\n",
            "| epoch  27 |   400/  434 batches | lr 0.000902 | ms/batch 112.41 | loss  4.46 | ppl    86.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 50.54s | valid loss  4.37 | valid ppl    78.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/  434 batches | lr 0.000919 | ms/batch 113.34 | loss  4.46 | ppl    86.45\n",
            "| epoch  28 |   400/  434 batches | lr 0.000934 | ms/batch 112.40 | loss  4.45 | ppl    85.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 50.64s | valid loss  4.35 | valid ppl    77.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/  434 batches | lr 0.000951 | ms/batch 113.19 | loss  4.44 | ppl    85.20\n",
            "| epoch  29 |   400/  434 batches | lr 0.000966 | ms/batch 112.90 | loss  4.44 | ppl    84.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 50.75s | valid loss  4.33 | valid ppl    76.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/  434 batches | lr 0.000983 | ms/batch 113.00 | loss  4.44 | ppl    84.35\n",
            "| epoch  30 |   400/  434 batches | lr 0.000998 | ms/batch 112.01 | loss  4.43 | ppl    83.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 50.48s | valid loss  4.33 | valid ppl    75.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/  434 batches | lr 0.000985 | ms/batch 113.18 | loss  4.43 | ppl    83.65\n",
            "| epoch  31 |   400/  434 batches | lr 0.000970 | ms/batch 112.78 | loss  4.41 | ppl    82.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 50.67s | valid loss  4.31 | valid ppl    74.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/  434 batches | lr 0.000953 | ms/batch 112.78 | loss  4.40 | ppl    81.69\n",
            "| epoch  32 |   400/  434 batches | lr 0.000938 | ms/batch 112.39 | loss  4.39 | ppl    80.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 50.55s | valid loss  4.29 | valid ppl    73.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/  434 batches | lr 0.000921 | ms/batch 112.79 | loss  4.37 | ppl    79.25\n",
            "| epoch  33 |   400/  434 batches | lr 0.000906 | ms/batch 112.02 | loss  4.36 | ppl    77.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 50.43s | valid loss  4.27 | valid ppl    71.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/  434 batches | lr 0.000889 | ms/batch 113.16 | loss  4.34 | ppl    76.65\n",
            "| epoch  34 |   400/  434 batches | lr 0.000874 | ms/batch 112.69 | loss  4.33 | ppl    75.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 50.65s | valid loss  4.26 | valid ppl    70.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/  434 batches | lr 0.000857 | ms/batch 112.61 | loss  4.31 | ppl    74.17\n",
            "| epoch  35 |   400/  434 batches | lr 0.000842 | ms/batch 112.73 | loss  4.29 | ppl    73.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 50.54s | valid loss  4.25 | valid ppl    70.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/  434 batches | lr 0.000825 | ms/batch 113.07 | loss  4.27 | ppl    71.73\n",
            "| epoch  36 |   400/  434 batches | lr 0.000810 | ms/batch 112.35 | loss  4.26 | ppl    71.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 50.54s | valid loss  4.24 | valid ppl    69.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/  434 batches | lr 0.000793 | ms/batch 112.87 | loss  4.24 | ppl    69.47\n",
            "| epoch  37 |   400/  434 batches | lr 0.000778 | ms/batch 112.77 | loss  4.23 | ppl    68.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 50.62s | valid loss  4.24 | valid ppl    69.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/  434 batches | lr 0.000761 | ms/batch 113.41 | loss  4.21 | ppl    67.23\n",
            "| epoch  38 |   400/  434 batches | lr 0.000746 | ms/batch 112.66 | loss  4.20 | ppl    66.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 50.72s | valid loss  4.22 | valid ppl    68.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/  434 batches | lr 0.000729 | ms/batch 112.82 | loss  4.18 | ppl    65.63\n",
            "| epoch  39 |   400/  434 batches | lr 0.000714 | ms/batch 112.39 | loss  4.17 | ppl    64.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 50.52s | valid loss  4.21 | valid ppl    67.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/  434 batches | lr 0.000697 | ms/batch 112.93 | loss  4.15 | ppl    63.61\n",
            "| epoch  40 |   400/  434 batches | lr 0.000682 | ms/batch 112.88 | loss  4.14 | ppl    62.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 50.66s | valid loss  4.22 | valid ppl    68.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/  434 batches | lr 0.000665 | ms/batch 112.96 | loss  4.12 | ppl    61.60\n",
            "| epoch  41 |   400/  434 batches | lr 0.000650 | ms/batch 113.15 | loss  4.11 | ppl    61.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 50.69s | valid loss  4.22 | valid ppl    68.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/  434 batches | lr 0.000633 | ms/batch 112.86 | loss  4.10 | ppl    60.14\n",
            "| epoch  42 |   400/  434 batches | lr 0.000618 | ms/batch 112.10 | loss  4.09 | ppl    59.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 50.46s | valid loss  4.20 | valid ppl    66.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/  434 batches | lr 0.000601 | ms/batch 112.59 | loss  4.07 | ppl    58.73\n",
            "| epoch  43 |   400/  434 batches | lr 0.000586 | ms/batch 111.51 | loss  4.06 | ppl    57.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 50.30s | valid loss  4.20 | valid ppl    66.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/  434 batches | lr 0.000569 | ms/batch 113.46 | loss  4.04 | ppl    57.08\n",
            "| epoch  44 |   400/  434 batches | lr 0.000554 | ms/batch 111.92 | loss  4.03 | ppl    56.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 50.56s | valid loss  4.20 | valid ppl    66.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/  434 batches | lr 0.000537 | ms/batch 112.39 | loss  4.02 | ppl    55.59\n",
            "| epoch  45 |   400/  434 batches | lr 0.000522 | ms/batch 112.85 | loss  4.01 | ppl    55.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 50.48s | valid loss  4.21 | valid ppl    67.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/  434 batches | lr 0.000505 | ms/batch 111.92 | loss  4.00 | ppl    54.54\n",
            "| epoch  46 |   400/  434 batches | lr 0.000490 | ms/batch 111.31 | loss  3.98 | ppl    53.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 50.13s | valid loss  4.20 | valid ppl    66.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/  434 batches | lr 0.000473 | ms/batch 112.98 | loss  3.97 | ppl    53.06\n",
            "| epoch  47 |   400/  434 batches | lr 0.000458 | ms/batch 112.03 | loss  3.96 | ppl    52.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 50.48s | valid loss  4.19 | valid ppl    66.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/  434 batches | lr 0.000441 | ms/batch 112.57 | loss  3.95 | ppl    51.69\n",
            "| epoch  48 |   400/  434 batches | lr 0.000426 | ms/batch 112.17 | loss  3.93 | ppl    51.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 50.41s | valid loss  4.19 | valid ppl    66.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/  434 batches | lr 0.000409 | ms/batch 112.74 | loss  3.92 | ppl    50.33\n",
            "| epoch  49 |   400/  434 batches | lr 0.000394 | ms/batch 111.51 | loss  3.91 | ppl    49.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 50.35s | valid loss  4.19 | valid ppl    66.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/  434 batches | lr 0.000377 | ms/batch 112.07 | loss  3.89 | ppl    49.04\n",
            "| epoch  50 |   400/  434 batches | lr 0.000362 | ms/batch 112.85 | loss  3.88 | ppl    48.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 50.42s | valid loss  4.20 | valid ppl    66.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   200/  434 batches | lr 0.000345 | ms/batch 112.58 | loss  3.87 | ppl    47.86\n",
            "| epoch  51 |   400/  434 batches | lr 0.000330 | ms/batch 111.87 | loss  3.85 | ppl    47.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 50.35s | valid loss  4.19 | valid ppl    66.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   200/  434 batches | lr 0.000313 | ms/batch 113.00 | loss  3.85 | ppl    46.81\n",
            "| epoch  52 |   400/  434 batches | lr 0.000298 | ms/batch 112.06 | loss  3.83 | ppl    46.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 50.45s | valid loss  4.19 | valid ppl    66.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |   200/  434 batches | lr 0.000281 | ms/batch 112.88 | loss  3.82 | ppl    45.49\n",
            "| epoch  53 |   400/  434 batches | lr 0.000266 | ms/batch 112.12 | loss  3.80 | ppl    44.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 50.46s | valid loss  4.19 | valid ppl    66.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   200/  434 batches | lr 0.000249 | ms/batch 112.55 | loss  3.79 | ppl    44.11\n",
            "| epoch  54 |   400/  434 batches | lr 0.000234 | ms/batch 111.67 | loss  3.77 | ppl    43.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 50.34s | valid loss  4.19 | valid ppl    65.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |   200/  434 batches | lr 0.000217 | ms/batch 112.30 | loss  3.76 | ppl    42.99\n",
            "| epoch  55 |   400/  434 batches | lr 0.000202 | ms/batch 111.45 | loss  3.75 | ppl    42.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 50.23s | valid loss  4.19 | valid ppl    65.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |   200/  434 batches | lr 0.000185 | ms/batch 112.49 | loss  3.73 | ppl    41.67\n",
            "| epoch  56 |   400/  434 batches | lr 0.000170 | ms/batch 111.89 | loss  3.72 | ppl    41.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 50.42s | valid loss  4.18 | valid ppl    65.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |   200/  434 batches | lr 0.000153 | ms/batch 113.07 | loss  3.70 | ppl    40.46\n",
            "| epoch  57 |   400/  434 batches | lr 0.000138 | ms/batch 112.05 | loss  3.69 | ppl    40.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 50.47s | valid loss  4.18 | valid ppl    65.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |   200/  434 batches | lr 0.000121 | ms/batch 112.55 | loss  3.67 | ppl    39.22\n",
            "| epoch  58 |   400/  434 batches | lr 0.000106 | ms/batch 112.61 | loss  3.66 | ppl    38.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 50.50s | valid loss  4.18 | valid ppl    65.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |   200/  434 batches | lr 0.000089 | ms/batch 112.55 | loss  3.64 | ppl    38.15\n",
            "| epoch  59 |   400/  434 batches | lr 0.000074 | ms/batch 111.99 | loss  3.63 | ppl    37.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 50.36s | valid loss  4.18 | valid ppl    65.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |   200/  434 batches | lr 0.000057 | ms/batch 113.08 | loss  3.61 | ppl    37.09\n",
            "| epoch  60 |   400/  434 batches | lr 0.000042 | ms/batch 112.04 | loss  3.61 | ppl    36.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 50.50s | valid loss  4.18 | valid ppl    65.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |   200/  434 batches | lr 0.000040 | ms/batch 113.37 | loss  3.59 | ppl    36.28\n",
            "| epoch  61 |   400/  434 batches | lr 0.000039 | ms/batch 111.34 | loss  3.59 | ppl    36.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 50.43s | valid loss  4.18 | valid ppl    65.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |   200/  434 batches | lr 0.000039 | ms/batch 112.21 | loss  3.58 | ppl    35.99\n",
            "| epoch  62 |   400/  434 batches | lr 0.000038 | ms/batch 112.16 | loss  3.59 | ppl    36.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 50.33s | valid loss  4.18 | valid ppl    65.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |   200/  434 batches | lr 0.000038 | ms/batch 112.96 | loss  3.58 | ppl    35.71\n",
            "| epoch  63 |   400/  434 batches | lr 0.000037 | ms/batch 111.88 | loss  3.58 | ppl    35.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 50.39s | valid loss  4.18 | valid ppl    65.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |   200/  434 batches | lr 0.000037 | ms/batch 112.02 | loss  3.57 | ppl    35.51\n",
            "| epoch  64 |   400/  434 batches | lr 0.000036 | ms/batch 111.79 | loss  3.57 | ppl    35.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 50.24s | valid loss  4.18 | valid ppl    65.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |   200/  434 batches | lr 0.000036 | ms/batch 112.45 | loss  3.56 | ppl    35.29\n",
            "| epoch  65 |   400/  434 batches | lr 0.000035 | ms/batch 111.83 | loss  3.57 | ppl    35.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 50.36s | valid loss  4.19 | valid ppl    65.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |   200/  434 batches | lr 0.000035 | ms/batch 111.66 | loss  3.56 | ppl    35.13\n",
            "| epoch  66 |   400/  434 batches | lr 0.000034 | ms/batch 111.99 | loss  3.56 | ppl    35.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 50.20s | valid loss  4.19 | valid ppl    65.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |   200/  434 batches | lr 0.000034 | ms/batch 112.59 | loss  3.55 | ppl    34.98\n",
            "| epoch  67 |   400/  434 batches | lr 0.000033 | ms/batch 112.33 | loss  3.56 | ppl    35.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 50.43s | valid loss  4.19 | valid ppl    66.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |   200/  434 batches | lr 0.000033 | ms/batch 112.60 | loss  3.55 | ppl    34.92\n",
            "| epoch  68 |   400/  434 batches | lr 0.000032 | ms/batch 112.59 | loss  3.56 | ppl    35.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 50.50s | valid loss  4.19 | valid ppl    66.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |   200/  434 batches | lr 0.000032 | ms/batch 112.17 | loss  3.55 | ppl    34.76\n",
            "| epoch  69 |   400/  434 batches | lr 0.000031 | ms/batch 112.87 | loss  3.56 | ppl    35.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 50.50s | valid loss  4.19 | valid ppl    66.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |   200/  434 batches | lr 0.000031 | ms/batch 112.74 | loss  3.55 | ppl    34.64\n",
            "| epoch  70 |   400/  434 batches | lr 0.000030 | ms/batch 112.00 | loss  3.55 | ppl    34.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 50.41s | valid loss  4.20 | valid ppl    66.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |   200/  434 batches | lr 0.000030 | ms/batch 112.42 | loss  3.54 | ppl    34.54\n",
            "| epoch  71 |   400/  434 batches | lr 0.000029 | ms/batch 112.42 | loss  3.55 | ppl    34.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 50.49s | valid loss  4.20 | valid ppl    66.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |   200/  434 batches | lr 0.000029 | ms/batch 112.01 | loss  3.54 | ppl    34.45\n",
            "| epoch  72 |   400/  434 batches | lr 0.000028 | ms/batch 111.57 | loss  3.55 | ppl    34.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 50.22s | valid loss  4.20 | valid ppl    66.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |   200/  434 batches | lr 0.000028 | ms/batch 112.19 | loss  3.54 | ppl    34.36\n",
            "| epoch  73 |   400/  434 batches | lr 0.000027 | ms/batch 112.66 | loss  3.54 | ppl    34.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 50.52s | valid loss  4.20 | valid ppl    66.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |   200/  434 batches | lr 0.000027 | ms/batch 112.70 | loss  3.53 | ppl    34.23\n",
            "| epoch  74 |   400/  434 batches | lr 0.000026 | ms/batch 112.72 | loss  3.54 | ppl    34.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 50.53s | valid loss  4.20 | valid ppl    66.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |   200/  434 batches | lr 0.000026 | ms/batch 112.49 | loss  3.53 | ppl    34.22\n",
            "| epoch  75 |   400/  434 batches | lr 0.000025 | ms/batch 111.86 | loss  3.54 | ppl    34.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 50.31s | valid loss  4.20 | valid ppl    66.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |   200/  434 batches | lr 0.000025 | ms/batch 113.18 | loss  3.53 | ppl    34.16\n",
            "| epoch  76 |   400/  434 batches | lr 0.000024 | ms/batch 112.14 | loss  3.54 | ppl    34.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 50.58s | valid loss  4.21 | valid ppl    67.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |   200/  434 batches | lr 0.000024 | ms/batch 112.23 | loss  3.53 | ppl    34.09\n",
            "| epoch  77 |   400/  434 batches | lr 0.000023 | ms/batch 112.64 | loss  3.54 | ppl    34.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 50.48s | valid loss  4.20 | valid ppl    66.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |   200/  434 batches | lr 0.000023 | ms/batch 112.36 | loss  3.53 | ppl    34.03\n",
            "| epoch  78 |   400/  434 batches | lr 0.000022 | ms/batch 112.19 | loss  3.54 | ppl    34.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 50.37s | valid loss  4.20 | valid ppl    67.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |   200/  434 batches | lr 0.000022 | ms/batch 112.69 | loss  3.53 | ppl    34.02\n",
            "| epoch  79 |   400/  434 batches | lr 0.000021 | ms/batch 112.28 | loss  3.54 | ppl    34.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 50.44s | valid loss  4.21 | valid ppl    67.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |   200/  434 batches | lr 0.000021 | ms/batch 112.71 | loss  3.52 | ppl    33.90\n",
            "| epoch  80 |   400/  434 batches | lr 0.000020 | ms/batch 112.23 | loss  3.54 | ppl    34.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 50.49s | valid loss  4.21 | valid ppl    67.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |   200/  434 batches | lr 0.000020 | ms/batch 112.76 | loss  3.52 | ppl    33.91\n",
            "| epoch  81 |   400/  434 batches | lr 0.000019 | ms/batch 112.04 | loss  3.54 | ppl    34.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 50.47s | valid loss  4.21 | valid ppl    67.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |   200/  434 batches | lr 0.000019 | ms/batch 113.16 | loss  3.52 | ppl    33.86\n",
            "| epoch  82 |   400/  434 batches | lr 0.000018 | ms/batch 111.71 | loss  3.53 | ppl    34.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 50.49s | valid loss  4.21 | valid ppl    67.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |   200/  434 batches | lr 0.000018 | ms/batch 112.86 | loss  3.52 | ppl    33.87\n",
            "| epoch  83 |   400/  434 batches | lr 0.000017 | ms/batch 112.49 | loss  3.53 | ppl    34.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 50.56s | valid loss  4.21 | valid ppl    67.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |   200/  434 batches | lr 0.000017 | ms/batch 112.95 | loss  3.52 | ppl    33.83\n",
            "| epoch  84 |   400/  434 batches | lr 0.000016 | ms/batch 112.81 | loss  3.53 | ppl    34.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 50.69s | valid loss  4.21 | valid ppl    67.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |   200/  434 batches | lr 0.000016 | ms/batch 112.76 | loss  3.52 | ppl    33.77\n",
            "| epoch  85 |   400/  434 batches | lr 0.000015 | ms/batch 112.94 | loss  3.53 | ppl    34.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 50.59s | valid loss  4.21 | valid ppl    67.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |   200/  434 batches | lr 0.000015 | ms/batch 112.63 | loss  3.52 | ppl    33.77\n",
            "| epoch  86 |   400/  434 batches | lr 0.000014 | ms/batch 112.55 | loss  3.53 | ppl    34.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 50.49s | valid loss  4.21 | valid ppl    67.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |   200/  434 batches | lr 0.000014 | ms/batch 113.63 | loss  3.52 | ppl    33.77\n",
            "| epoch  87 |   400/  434 batches | lr 0.000013 | ms/batch 111.81 | loss  3.53 | ppl    34.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 50.59s | valid loss  4.21 | valid ppl    67.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |   200/  434 batches | lr 0.000013 | ms/batch 113.32 | loss  3.52 | ppl    33.81\n",
            "| epoch  88 |   400/  434 batches | lr 0.000012 | ms/batch 112.06 | loss  3.53 | ppl    34.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 50.60s | valid loss  4.21 | valid ppl    67.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |   200/  434 batches | lr 0.000012 | ms/batch 112.64 | loss  3.52 | ppl    33.71\n",
            "| epoch  89 |   400/  434 batches | lr 0.000011 | ms/batch 112.16 | loss  3.53 | ppl    34.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 50.47s | valid loss  4.21 | valid ppl    67.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |   200/  434 batches | lr 0.000011 | ms/batch 112.02 | loss  3.52 | ppl    33.78\n",
            "| epoch  90 |   400/  434 batches | lr 0.000010 | ms/batch 113.10 | loss  3.53 | ppl    34.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 50.52s | valid loss  4.21 | valid ppl    67.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |   200/  434 batches | lr 0.000010 | ms/batch 112.96 | loss  3.52 | ppl    33.69\n",
            "| epoch  91 |   400/  434 batches | lr 0.000009 | ms/batch 112.46 | loss  3.53 | ppl    34.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 50.58s | valid loss  4.21 | valid ppl    67.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |   200/  434 batches | lr 0.000009 | ms/batch 112.95 | loss  3.52 | ppl    33.77\n",
            "| epoch  92 |   400/  434 batches | lr 0.000008 | ms/batch 111.87 | loss  3.53 | ppl    34.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 50.41s | valid loss  4.21 | valid ppl    67.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |   200/  434 batches | lr 0.000008 | ms/batch 112.55 | loss  3.52 | ppl    33.69\n",
            "| epoch  93 |   400/  434 batches | lr 0.000007 | ms/batch 112.49 | loss  3.53 | ppl    33.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 50.46s | valid loss  4.21 | valid ppl    67.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |   200/  434 batches | lr 0.000007 | ms/batch 113.04 | loss  3.52 | ppl    33.63\n",
            "| epoch  94 |   400/  434 batches | lr 0.000006 | ms/batch 111.87 | loss  3.53 | ppl    33.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 50.51s | valid loss  4.21 | valid ppl    67.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |   200/  434 batches | lr 0.000006 | ms/batch 112.81 | loss  3.52 | ppl    33.69\n",
            "| epoch  95 |   400/  434 batches | lr 0.000005 | ms/batch 112.71 | loss  3.53 | ppl    33.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 50.54s | valid loss  4.21 | valid ppl    67.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |   200/  434 batches | lr 0.000005 | ms/batch 112.50 | loss  3.52 | ppl    33.67\n",
            "| epoch  96 |   400/  434 batches | lr 0.000004 | ms/batch 111.81 | loss  3.53 | ppl    34.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 50.40s | valid loss  4.21 | valid ppl    67.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |   200/  434 batches | lr 0.000004 | ms/batch 112.69 | loss  3.52 | ppl    33.70\n",
            "| epoch  97 |   400/  434 batches | lr 0.000003 | ms/batch 112.44 | loss  3.53 | ppl    33.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 50.53s | valid loss  4.21 | valid ppl    67.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |   200/  434 batches | lr 0.000003 | ms/batch 112.56 | loss  3.52 | ppl    33.64\n",
            "| epoch  98 |   400/  434 batches | lr 0.000002 | ms/batch 112.37 | loss  3.52 | ppl    33.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 50.46s | valid loss  4.21 | valid ppl    67.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |   200/  434 batches | lr 0.000002 | ms/batch 112.82 | loss  3.51 | ppl    33.54\n",
            "| epoch  99 |   400/  434 batches | lr 0.000001 | ms/batch 112.64 | loss  3.52 | ppl    33.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 50.65s | valid loss  4.21 | valid ppl    67.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |   200/  434 batches | lr 0.000001 | ms/batch 112.39 | loss  3.51 | ppl    33.50\n",
            "| epoch 100 |   400/  434 batches | lr 0.000000 | ms/batch 112.48 | loss  3.52 | ppl    33.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 50.43s | valid loss  4.21 | valid ppl    67.57\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not important"
      ],
      "metadata": {
        "id": "ON8LmbvYdMGU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndJCTLgBuQHp"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  num_classes = train_loader.dataset.num_categories\n",
        "  l = 0\n",
        "  acc = 0\n",
        "  pbar = tqdm(total = len(train_loader),position=0,leave=True)\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    x, output_shifted = data\n",
        "    preds = model(x.to(device), output_shifted.to(device))       \n",
        "    loss = criterion(preds.view(-1,preds.size(-1)), target.view(-1),reduction=\"mean\")\n",
        "    loss.backward() \n",
        "    #nn.utils.clip_grad_norm(model.parameters(), clip)       \n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      current_loss = loss.item()\n",
        "      l+= loss.item()\n",
        "      acc+=(preds.argmax(dim=-1) == target).float().mean().item()\n",
        "    pbar.set_description('training_step {} loss:{:3f}'.format(batch_idx,current_loss))\n",
        "    pbar.update()\n",
        "  acc = 100. * acc / (len(train_loader))\n",
        "  l = l/len(train_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format('train',l,acc))\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOzqXY-WwNlr"
      },
      "source": [
        "def test( model, device, test_loader,criterion,mode='eval'):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  num_classes=test_loader.dataset.num_categories\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          target = target.to(device)\n",
        "          x, output_shifted = data\n",
        "          output = model(x.to(device), output_shifted.to(device))\n",
        "          test_loss += criterion(output.view(-1,output.size(-1)),\\\n",
        "                                        target.view(-1)).item()          \n",
        "          correct += (output.argmax(dim=-1) == target).float().mean().item()\n",
        "\n",
        "  loss = test_loss/len(test_loader)\n",
        "  acc = 100. * correct / len(test_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format(mode,loss,acc))\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raohC_7iRk0N"
      },
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(low=1, high=self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        labels_shifted = labels.roll(1,0)\n",
        "        labels_shifted[0] = torch.tensor(0)\n",
        "        return (inp_data,labels_shifted), labels"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8jEEssRQX5"
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.determinstic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dataset = partial(ReverseDataset, 10, 16)\n",
        "    train_loader = data.DataLoader(dataset(50000), batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    \n",
        "    val_loader   = data.DataLoader(dataset(1000), batch_size=test_batch_size)\n",
        "    test_loader  = data.DataLoader(dataset(10000), batch_size=test_batch_size)\n",
        "    model = UnetTransformer(n_blocks=n_blocks,input_dim=train_loader.dataset.num_categories,\\\n",
        "                            n_heads=n_heads,d_model = h_dims,num_classes=\\\n",
        "                            train_loader.dataset.num_categories,dim_feedforward = h_dims,\\\n",
        "                            dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "    criterion =  F.cross_entropy\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "       train(model, device, train_loader, optimizer,criterion)\n",
        "       test(model, device, val_loader,criterion)\n",
        "        \n",
        "    torch.save(model.state_dict(), \"model.h5\")\n",
        "    print('------------testing--------------')\n",
        "    test(model, device, test_loader,criterion,mode='test')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #main()"
      ],
      "execution_count": 23,
      "outputs": []
    }
  ]
}