{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMFIUBtAqxcwks6zhNB+JAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDayoub/U-transformer/blob/main/U_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsHozpzlIsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62abcdda-991e-4848-ce26-ef0af5989fe6"
      },
      "source": [
        "'''\n",
        "Import required libraries\n",
        "'''\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Tuple\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrbb2GkSKiJ"
      },
      "source": [
        "# batch_size = 128          \n",
        "# test_batch_size = 128   \n",
        "epochs = 100             \n",
        "lr = 5e-4               \n",
        "seed = 42               \n",
        "h_dims = 1024\n",
        "n_heads = 16\n",
        "n_blocks = 2\n",
        "dropout = 0.2\n",
        "clip = 0.5\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "bptt = 256\n",
        "d_model = 400"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvxLamYx-zF"
      },
      "source": [
        "'''\n",
        "This code is adapted from \n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "'''\n",
        "class Embedding_with_PosEncoding(nn.Module):\n",
        "  def __init__(self,input_dim,d_model, max_len=5000,dropout=0):\n",
        "    '''\n",
        "    Args:\n",
        "      d_model: hidden space dimentionality for Embedding\n",
        "      input_dim: input space dimentionality\n",
        "      max_len: maximum length of an input sequence\n",
        "      drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(Embedding_with_PosEncoding,self).__init__()\n",
        "    self.emb = nn.Embedding(input_dim,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    pe = pe.unsqueeze(0)\n",
        "    # register_buffer is used to save and retrain parameters which don't need to train\n",
        "    self.register_buffer('pe', pe, persistent=False) \n",
        "  def forward(self,x):\n",
        "    seq_len = x.size(1)\n",
        "    x = self.emb(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + self.pe[:, :seq_len]\n",
        "    return x\n",
        "  def get_pe(self):\n",
        "    return self.pe"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2k37K8dSjZB"
      },
      "source": [
        "def test_positional_encoding():\n",
        "  batch_dim,seq_len,input_dim= (15,10,10)\n",
        "  d_model = 100\n",
        "  max_len =100\n",
        "  x = torch.randint(low=0, high=10,size=(batch_dim,seq_len))\n",
        "  pos_encoder = Embedding_with_PosEncoding(input_dim,d_model,max_len)\n",
        "  pe = pos_encoder.get_pe()\n",
        "  res = pos_encoder(x)\n",
        "  assert res.shape ==  torch.Size([batch_dim,seq_len,d_model])\n",
        "  assert pe.shape == torch.Size([1, max_len, d_model])\n",
        "test_positional_encoding()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NzV-cFUbdz"
      },
      "source": [
        "def scaled_dot_product(query,key,values,mask=None,scale=True):\n",
        "  '''\n",
        "      Args:\n",
        "        query: tensor of queries\n",
        "        key : tensor of keys\n",
        "        value: tensor of value\n",
        "        mask (numpy.ndarray): attention-mask, used to perform self attention when required\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "  '''\n",
        "  if scale:\n",
        "    depth = query.shape[-1] ** 0.5\n",
        "  else:\n",
        "    depth = 1\n",
        "  dots = torch.matmul(query,torch.swapaxes(key,-1,-2))/depth\n",
        "  if mask is not None:\n",
        "    dots = torch.where(mask,dots,torch.full_like(dots, -9e15))\n",
        "  logsumexp = torch.logsumexp(dots, axis=-1, keepdims=True)\n",
        "  dots = torch.exp(dots - logsumexp)\n",
        "  attention = torch.matmul(dots, values)\n",
        "  return attention\n",
        "def dot_product_self_attention(q, k, v,device=device):\n",
        "  '''\n",
        "    Args:\n",
        "        q: queries.\n",
        "        k: keys.\n",
        "        v: values.\n",
        "    Returns:\n",
        "        masked dot product self attention tensor.  \n",
        "  '''\n",
        "  mask_size = q.shape[-2]\n",
        "  mask = torch.tril(torch.ones((1, mask_size, mask_size), dtype=torch.bool), diagonal=0).to(device)        \n",
        "  return scaled_dot_product(q, k, v, mask)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA0CRdjiU8H"
      },
      "source": [
        "class QKV(nn.Module):\n",
        "  '''\n",
        "  takes as input a tensor of shape (batch_size,seq_len,d_model)\n",
        "  returns:\n",
        "  three tensors q,k,v of shape (batch_size,n_heads,seq_len,d_model//n_heads)\n",
        "  '''\n",
        "\n",
        "  def __init__(self,n_heads,d_model):\n",
        "    '''\n",
        "      Args:\n",
        "        n_heads: number of heads used in multihead attention\n",
        "        d_model: hidden space dimensions\n",
        "    '''\n",
        "    assert d_model%n_heads==0,'d_models should be divisible by n_heads'\n",
        "    super(QKV,self).__init__()\n",
        "    self.qvk = nn.Linear(in_features=d_model,out_features=3*d_model)\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_heads = d_model//n_heads\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len,d_model = x.shape\n",
        "    x = self.qvk(x)\n",
        "    x = x.reshape(batch_size,seq_len,self.n_heads,3*self.d_heads)\n",
        "    x = x.permute(0,2,1,3)\n",
        "    q,k,v = x.chunk(3,dim=-1)\n",
        "    return q,k,v\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JEfLDB7oW_"
      },
      "source": [
        "def test_QKV():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  x = torch.randn(batch_dim,seq_len, d_model).to(device)\n",
        "  qkv = QKV(n_heads=n_heads,d_model=d_model).to(device)\n",
        "  q,k,v = qkv(x)\n",
        "  assert q.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert k.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert v.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "test_QKV()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CR3TtPk2wnr"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  '''\n",
        "  This class implements mulithead attention\n",
        "  '''\n",
        "  def __init__(self,d_model,causal_attention=False):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        causal_attention: boolean whether to use attention or causal attention \n",
        "    '''\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.o = nn.Linear(in_features=d_model,out_features=d_model)\n",
        "    self.causal_attention = causal_attention \n",
        "\n",
        "  def forward(self,q,k,v):\n",
        "    batch_size,n_heads,seq_len,d_heads = q.shape\n",
        "    if self.causal_attention:\n",
        "      atten =  dot_product_self_attention(q, k, v)\n",
        "    else:\n",
        "      atten = scaled_dot_product(q,k,v)\n",
        "    atten = atten.permute(0,2,1,3)\n",
        "    atten = atten.reshape(batch_size,seq_len,self.d_model)\n",
        "    res = self.o(atten)\n",
        "    return res\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTh48ZAH8mSw"
      },
      "source": [
        "def test_MultiheadAttention():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  att = MultiheadAttention(d_model,causal_attention=False).to(device)\n",
        "  causal_att = MultiheadAttention(d_model,causal_attention=True).to(device)\n",
        "  x = torch.randn(batch_dim, n_heads, seq_len,3,d_model//n_heads).to(device)\n",
        "  q,k,v = x[:,:,:,0,:],x[:,:,:,1,:],x[:,:,:,2,:]\n",
        "  o1 = att(q,k,v)\n",
        "  o2 = causal_att(q,k,v)\n",
        "  assert o1.shape ==  torch.Size([batch_dim, seq_len,d_model])\n",
        "  assert o2.shape ==  torch.Size([batch_dim,  seq_len,d_model])\n",
        "test_MultiheadAttention()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTC9-skP2ory"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  '''\n",
        "  This class implements encoder block\n",
        "  '''\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(EncoderBlock,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.qkv =  QKV(n_heads=n_heads,d_model=d_model)\n",
        "    self.attention = MultiheadAttention(d_model=d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = self.feedforward(x2)\n",
        "    x4 = self.norm2(self.dropout(x3)+x2)\n",
        "    return x4\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWmFwuHFO46"
      },
      "source": [
        "def reshape_tensor(x,n_heads):\n",
        "  '''\n",
        "    Args:\n",
        "      x: tensor of shape (batch_size,seq_len,d_model)\n",
        "      n_heads: number of heads in mutlihead attention\n",
        "    Returns:\n",
        "      reshaped tensor of shape (batch_size,n_heads,seq_len,d_model//n_heads)    \n",
        "  '''\n",
        "  batch_size,seq_len,d_model = x.shape\n",
        "  x = x.reshape(batch_size,seq_len,n_heads,d_model//n_heads)\n",
        "  x = x.permute(0,2,1,3)\n",
        "  return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  '''\n",
        "    This class implements decoder block\n",
        "  '''\n",
        "\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.qkv = QKV(n_heads,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.causal_attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0,skip_con):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.causal_attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = reshape_tensor(x2,self.n_heads)\n",
        "    skip_con = reshape_tensor(skip_con,self.n_heads)\n",
        "    x4 = self.attention(x3,skip_con,skip_con)\n",
        "    x5 = self.norm2(x2+self.dropout(x4))\n",
        "    x6 = self.feedforward(x5)\n",
        "    x7 = self.norm3(self.dropout(x6)+x5)\n",
        "    return x7"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG1NMhDKAvw"
      },
      "source": [
        "class UnetTransformer(nn.Module):\n",
        "  '''\n",
        "    This class implements unet transformer\n",
        "  '''\n",
        "  def __init__(self,n_blocks,n_tokens,n_heads,d_model,dim_feedforward,dropout=0.0):\n",
        "\n",
        "    '''\n",
        "      Args:\n",
        "        n_blocks: number of encoder/decoder blocks\n",
        "        n_tokens: Dimensionality of the input space\n",
        "        n_heads: number of heads in MultiHeadAttention\n",
        "        d_model: Dimensionality of the embedding space\n",
        "        num_classes: Dimensionality of the output space\n",
        "        dim_feedforward:  Dimensionality of the hidden layer in the MLP \n",
        "\n",
        "\n",
        "    '''\n",
        "    super(UnetTransformer,self).__init__()\n",
        "    self.n_blocks = n_blocks\n",
        "    self.pos_enc = Embedding_with_PosEncoding(n_tokens,d_model,dropout=dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['enc_'+str(i)] = EncoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['dec_'+str(i)] = DecoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    self.output_layer = nn.Sequential( nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, n_tokens)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "    x_encoded = self.pos_enc(x)\n",
        "    layers = vars(self)['_modules']\n",
        "    stack = [x_encoded]\n",
        "    x = layers['enc_0'](x_encoded)\n",
        "    for i in range(1,self.n_blocks):\n",
        "      stack.append(x)\n",
        "      x = layers['enc_'+str(i)](x)\n",
        "    stack.append(x)\n",
        "    x = layers['dec_0'](x,stack.pop(0))\n",
        "    for i in range(1,self.n_blocks):\n",
        "      x = layers['dec_'+str(i)](x,stack.pop(0))\n",
        "    return self.output_layer(x)\n",
        "\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset"
      ],
      "metadata": {
        "id": "BcWK--CdaBXU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "c2n815-3aGA4"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, source.shape[1] - 1 - i)\n",
        "    data = source[:,i:i+seq_len]\n",
        "    target = source[:,i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "get_batch(train_data,0)[0].shape"
      ],
      "metadata": {
        "id": "0CGLbgM1aS2m",
        "outputId": "4581075e-0164-43d1-b073-a8b9214d06a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "print('n_tokens {}'.format(len(vocab)))\n",
        "model = UnetTransformer(n_blocks=n_blocks,n_tokens=ntokens,\\\n",
        "                        n_heads=n_heads, d_model = d_model,dim_feedforward = h_dims,\\\n",
        "                        dropout=dropout).to(device)\n",
        "pytorch_total_params = sum(p.numel()\n",
        "                        for p in model.parameters() if p.requires_grad)\n",
        "print('-' * 89)\n",
        "print(\n",
        "    '#'*12+f\" Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters for {epochs:3d} epochs \"+'#'*12)\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8us_WwAIaup5",
        "outputId": "c77a6add-62f9-44f9-bcac-ba7f17aeae63"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens 33243\n",
            "-----------------------------------------------------------------------------------------\n",
            "############ Training model with 32.97M trainable parameters for 100 epochs ############\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicOpt:\n",
        "    def __init__(self, optimizer, schedular):\n",
        "        self.optimizer = optimizer\n",
        "        self.schedular = schedular\n",
        "        self._scalar = 1\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    @property\n",
        "    def lr(self):\n",
        "      return self.optimizer.param_groups[0]['lr']*self._scalar\n",
        "\n",
        "    @property\n",
        "    def scalar(self):\n",
        "        return self._scalar\n",
        "\n",
        "    @scalar.setter\n",
        "    def scalar(self, scalar):\n",
        "        self._scalar = scalar\n",
        "\n",
        "\n",
        "    def schedule_step(self, val_loss):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class linearcycleWarmup(BasicOpt):\n",
        "    def __init__(self, optimizer, schedular, *args, **kwargs):\n",
        "        super().__init__(optimizer=optimizer, schedular=schedular)\n",
        "        self.use_scheduler = True\n",
        "\n",
        "       \n",
        "    def step(self):\n",
        "        lr_s = [p['lr'] for p in self.optimizer.param_groups]\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr']  = p['lr']*self._scalar             \n",
        "        self.optimizer.step()\n",
        "        for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = lr_s[idx]\n",
        "        try:\n",
        "          if self.use_scheduler:\n",
        "            self.schedular.step()\n",
        "        except Exception as e:\n",
        "          self.use_scheduler = False\n",
        "          for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = 0.00000088\n",
        "\n",
        "\n",
        "\n",
        "    def schedule_step(self, *args):\n",
        "        pass\n",
        "steps_per_epoch = len(train_data)//bptt+1\n",
        "total_steps = epochs*(steps_per_epoch)\n",
        "opt_args = {\n",
        "    'lr': 0,\n",
        "    'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "}\n",
        "\n",
        "linear_args = {\n",
        "    'total_steps': total_steps,\n",
        "    'pct_start': 0.3, 'anneal_strategy': 'linear',\n",
        "    'three_phase': True, 'max_lr': 1e-3\n",
        "}\n",
        "opt = torch.optim.RAdam(model.parameters(),\n",
        "                        **opt_args)\n",
        "schedular_args = linear_args\n",
        "schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)"
      ],
      "metadata": {
        "id": "yk9NtX6gf9qw"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = train_data.shape[1] // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(1) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        batch_size = data.size(1)\n",
        "        # if batch_size != bptt:  # only on last batch\n",
        "        #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = optimizer.lr\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(1) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            batch_size = data.size(1)\n",
        "            # if batch_size != bptt:\n",
        "            #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (eval_data.size(1) - 1)"
      ],
      "metadata": {
        "id": "Gspv87NsaWha"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "IHvZbtTJaa-4",
        "outputId": "62c535f4-1aa9-4dbb-bab5-2a397a64e363"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  434 batches | lr 0.000001 | ms/batch 111.58 | loss  7.64 | ppl  2087.41\n",
            "| epoch   1 |   400/  434 batches | lr 0.000001 | ms/batch 111.18 | loss  7.08 | ppl  1191.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 50.01s | valid loss  6.52 | valid ppl   678.87\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c9ddfe8311b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-c74b48b18818>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# if batch_size != bptt:  # only on last batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#     src_mask = src_mask[:batch_size, :batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-a61900dda717>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enc_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dec_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-cea9a3d8335b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x0)\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e9cbe977bfe1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcausal_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0matten\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdot_product_self_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0matten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-52e847b828b8>\u001b[0m in \u001b[0;36mdot_product_self_attention\u001b[0;34m(q, k, v, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m   '''\n\u001b[1;32m     30\u001b[0m   \u001b[0mmask_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_dot_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not important"
      ],
      "metadata": {
        "id": "ON8LmbvYdMGU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndJCTLgBuQHp"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  num_classes = train_loader.dataset.num_categories\n",
        "  l = 0\n",
        "  acc = 0\n",
        "  pbar = tqdm(total = len(train_loader),position=0,leave=True)\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    x, output_shifted = data\n",
        "    preds = model(x.to(device), output_shifted.to(device))       \n",
        "    loss = criterion(preds.view(-1,preds.size(-1)), target.view(-1),reduction=\"mean\")\n",
        "    loss.backward() \n",
        "    #nn.utils.clip_grad_norm(model.parameters(), clip)       \n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      current_loss = loss.item()\n",
        "      l+= loss.item()\n",
        "      acc+=(preds.argmax(dim=-1) == target).float().mean().item()\n",
        "    pbar.set_description('training_step {} loss:{:3f}'.format(batch_idx,current_loss))\n",
        "    pbar.update()\n",
        "  acc = 100. * acc / (len(train_loader))\n",
        "  l = l/len(train_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format('train',l,acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOzqXY-WwNlr"
      },
      "source": [
        "def test( model, device, test_loader,criterion,mode='eval'):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  num_classes=test_loader.dataset.num_categories\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          target = target.to(device)\n",
        "          x, output_shifted = data\n",
        "          output = model(x.to(device), output_shifted.to(device))\n",
        "          test_loss += criterion(output.view(-1,output.size(-1)),\\\n",
        "                                        target.view(-1)).item()          \n",
        "          correct += (output.argmax(dim=-1) == target).float().mean().item()\n",
        "\n",
        "  loss = test_loss/len(test_loader)\n",
        "  acc = 100. * correct / len(test_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format(mode,loss,acc))\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raohC_7iRk0N"
      },
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(low=1, high=self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        labels_shifted = labels.roll(1,0)\n",
        "        labels_shifted[0] = torch.tensor(0)\n",
        "        return (inp_data,labels_shifted), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8jEEssRQX5"
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.determinstic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dataset = partial(ReverseDataset, 10, 16)\n",
        "    train_loader = data.DataLoader(dataset(50000), batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    \n",
        "    val_loader   = data.DataLoader(dataset(1000), batch_size=test_batch_size)\n",
        "    test_loader  = data.DataLoader(dataset(10000), batch_size=test_batch_size)\n",
        "    model = UnetTransformer(n_blocks=n_blocks,input_dim=train_loader.dataset.num_categories,\\\n",
        "                            n_heads=n_heads,d_model = h_dims,num_classes=\\\n",
        "                            train_loader.dataset.num_categories,dim_feedforward = h_dims,\\\n",
        "                            dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "    criterion =  F.cross_entropy\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "       train(model, device, train_loader, optimizer,criterion)\n",
        "       test(model, device, val_loader,criterion)\n",
        "        \n",
        "    torch.save(model.state_dict(), \"model.h5\")\n",
        "    print('------------testing--------------')\n",
        "    test(model, device, test_loader,criterion,mode='test')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}