{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOXdglJzx8qkikgPwbGz3Rv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDayoub/U-transformer/blob/main/U_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NsHozpzlIsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3bec36-601c-4ff2-a11c-d0b744c2995a"
      },
      "source": [
        "'''\n",
        "Import required libraries\n",
        "'''\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from functools import partial\n",
        "import math\n",
        "from typing import Tuple\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrbb2GkSKiJ"
      },
      "source": [
        "# batch_size = 128          \n",
        "# test_batch_size = 128   \n",
        "epochs = 100             \n",
        "lr = 5e-4               \n",
        "seed = 42               \n",
        "h_dims = 1024\n",
        "n_heads = 16\n",
        "n_blocks = 2\n",
        "dropout = 0.1\n",
        "out_dropout = 0.4\n",
        "emb_dropout = 0.1\n",
        "clip = 0.5\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "bptt = 256\n",
        "d_model = 400"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "set_seed(seed)"
      ],
      "metadata": {
        "id": "y8wnlsR8KR5p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConv1d(nn.Module):\n",
        "  def __init__(self, d_model, kernel_size=3, dilation=1, **kwargs):\n",
        "    pad = (kernel_size - 1) * dilation\n",
        "    self.conv = nn.Conv1d(d_model, d_model, kernel_size, padding=pad, dilation=dilation, groups=d_model, **kwargs)\n",
        "  def forward(self,x):\n",
        "    ''''\n",
        "    x has shape of batch_size, seq_len, d_model\n",
        "    '''\n",
        "    x = x.transpose(2,1) # now shape is [batch_size, d_model, seq_len]\n",
        "    x = self.conv1(x)\n",
        "    x = x[:, :, :-self.conv1.padding[0]]\n",
        "    x = x.transpose(2,1)\n",
        "    return x.contiguous()\n"
      ],
      "metadata": {
        "id": "cFR8anxGJTDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # register_buffer is used to save and retrain parameters which don't need to train\n",
        "        self.register_buffer('pe', pe, persistent=False) \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "inWGWGwa6z-_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvxLamYx-zF"
      },
      "source": [
        "# '''\n",
        "# This code is adapted from \n",
        "# https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "# '''\n",
        "# class Embedding_with_PosEncoding(nn.Module):\n",
        "#   def __init__(self,input_dim,d_model, max_len=5000,dropout=0):\n",
        "#     '''\n",
        "#     Args:\n",
        "#       d_model: hidden space dimentionality for Embedding\n",
        "#       input_dim: input space dimentionality\n",
        "#       max_len: maximum length of an input sequence\n",
        "#       drop: probability of an element to be zeroed\n",
        "#     '''\n",
        "#     super(Embedding_with_PosEncoding,self).__init__()\n",
        "#     self.emb = nn.Embedding(input_dim,d_model)\n",
        "#     self.dropout = nn.Dropout(p=dropout)\n",
        "#     pe = torch.zeros(max_len, d_model)\n",
        "#     position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#     div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "#     pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#     pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#     pe = pe.unsqueeze(0)\n",
        "#     # register_buffer is used to save and retrain parameters which don't need to train\n",
        "#     self.register_buffer('pe', pe, persistent=False) \n",
        "#   def forward(self,x):\n",
        "#     seq_len = x.size(1)\n",
        "#     x = self.emb(x)\n",
        "#     x = self.dropout(x)\n",
        "#     x = x + self.pe[:, :seq_len]\n",
        "#     return x\n",
        "#   def get_pe(self):\n",
        "#     return self.pe"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2k37K8dSjZB"
      },
      "source": [
        "# def test_positional_encoding():\n",
        "#   batch_dim,seq_len,input_dim= (15,10,10)\n",
        "#   d_model = 100\n",
        "#   max_len =100\n",
        "#   x = torch.randint(low=0, high=10,size=(batch_dim,seq_len))\n",
        "#   pos_encoder = Embedding_with_PosEncoding(input_dim,d_model,max_len)\n",
        "#   pe = pos_encoder.get_pe()\n",
        "#   res = pos_encoder(x)\n",
        "#   assert res.shape ==  torch.Size([batch_dim,seq_len,d_model])\n",
        "#   assert pe.shape == torch.Size([1, max_len, d_model])\n",
        "# test_positional_encoding()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9NzV-cFUbdz"
      },
      "source": [
        "def scaled_dot_product(query,key,values,mask=None,scale=True):\n",
        "  '''\n",
        "      Args:\n",
        "        query: tensor of queries\n",
        "        key : tensor of keys\n",
        "        value: tensor of value\n",
        "        mask (numpy.ndarray): attention-mask, used to perform self attention when required\n",
        "        scale (bool): whether to scale the dot product of the query and transposed key\n",
        "  '''\n",
        "  if scale:\n",
        "    depth = query.shape[-1] ** 0.5\n",
        "  else:\n",
        "    depth = 1\n",
        "  dots = torch.matmul(query,torch.swapaxes(key,-1,-2))/depth\n",
        "  if mask is not None:\n",
        "    dots = torch.where(mask,dots,torch.full_like(dots, -9e15))\n",
        "  logsumexp = torch.logsumexp(dots, axis=-1, keepdims=True)\n",
        "  dots = torch.exp(dots - logsumexp)\n",
        "  #dots = torch.sigmoid(dots)\n",
        "  attention = torch.matmul(dots, values)\n",
        "  return attention\n",
        "def dot_product_self_attention(q, k, v,device=device):\n",
        "  '''\n",
        "    Args:\n",
        "        q: queries.\n",
        "        k: keys.\n",
        "        v: values.\n",
        "    Returns:\n",
        "        masked dot product self attention tensor.  \n",
        "  '''\n",
        "  mask_size = q.shape[-2]\n",
        "  mask = torch.tril(torch.ones((1, mask_size, mask_size), dtype=torch.bool), diagonal=0).to(device)        \n",
        "  return scaled_dot_product(q, k, v, mask)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class se_block(nn.Module):\n",
        "  def __init__(self, n_heads):\n",
        "    self.n_heads = n_heads\n",
        "    self.lin1 = nn.Linear(n_heads, n_heads//4)\n",
        "    self.lin2 = nn.Linear(n_heads//4, n_heads)\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    '''\n",
        "    x has shape batch_size,seq_len,self.n_heads,self.d_heads\n",
        "    '''\n",
        "    inputs = x\n",
        "    x = torch.mean(x, dim=-1, keepdim=False)\n",
        "    x = self.relu(self.lin1(x))\n",
        "    x = self.sig(self.lin2(x))\n",
        "    x = inputs*x.unsqueeze(-1)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LclHOov_BxYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gA0CRdjiU8H"
      },
      "source": [
        "class QKV(nn.Module):\n",
        "  '''\n",
        "  takes as input a tensor of shape (batch_size,seq_len,d_model)\n",
        "  returns:\n",
        "  three tensors q,k,v of shape (batch_size,n_heads,seq_len,d_model//n_heads)\n",
        "  '''\n",
        "\n",
        "  def __init__(self,n_heads,d_model):\n",
        "    '''\n",
        "      Args:\n",
        "        n_heads: number of heads used in multihead attention\n",
        "        d_model: hidden space dimensions\n",
        "    '''\n",
        "    assert d_model%n_heads==0,'d_models should be divisible by n_heads'\n",
        "    super(QKV,self).__init__()\n",
        "    #self.qvk = nn.Linear(in_features=d_model,out_features=2*d_model)\n",
        "    self.q = nn.Sequential(nn.Linear(in_features=d_model,out_features=d_model//2, bias=False),\\\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(in_features=d_model//2,out_features=d_model,bias=False))\n",
        "    self.k = nn.Sequential(nn.Linear(in_features=d_model,out_features=d_model//2, bias=False),\\\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(in_features=d_model//2,out_features=d_model,bias=False))\n",
        "    self.v = nn.Sequential(nn.Linear(in_features=d_model,out_features=d_model//2),\\\n",
        "                             nn.ReLU(),\n",
        "                             nn.Linear(in_features=d_model//2,out_features=d_model))\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_heads = d_model//n_heads\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_len,d_model = x.shape\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "    x = torch.cat([q,k,v],dim=-1)\n",
        "    x = x.reshape(batch_size,seq_len,self.n_heads,3*self.d_heads)\n",
        "    x = x.permute(0,2,1,3)\n",
        "    q,k,v = x.chunk(3,dim=-1)\n",
        "    q = se_block(q)\n",
        "    k = se_block(k)\n",
        "    v = se_block(v)\n",
        "    return q,k,v\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9JEfLDB7oW_"
      },
      "source": [
        "def test_QKV():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  x = torch.randn(batch_dim,seq_len, d_model).to(device)\n",
        "  qkv = QKV(n_heads=n_heads,d_model=d_model).to(device)\n",
        "  q,k,v = qkv(x)\n",
        "  assert q.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert k.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "  assert v.shape ==  torch.Size([batch_dim, n_heads, seq_len,d_model//n_heads])\n",
        "test_QKV()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CR3TtPk2wnr"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  '''\n",
        "  This class implements mulithead attention\n",
        "  '''\n",
        "  def __init__(self,d_model,causal_attention=False):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        causal_attention: boolean whether to use attention or causal attention \n",
        "    '''\n",
        "    super(MultiheadAttention,self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.o = nn.Linear(in_features=d_model,out_features=d_model)\n",
        "    self.causal_attention = causal_attention \n",
        "\n",
        "  def forward(self,q,k,v):\n",
        "    batch_size,n_heads,seq_len,d_heads = q.shape\n",
        "    if self.causal_attention:\n",
        "      atten =  dot_product_self_attention(q, k, v)\n",
        "    else:\n",
        "      atten = scaled_dot_product(q,k,v)\n",
        "    atten = atten.permute(0,2,1,3)\n",
        "    atten = atten.reshape(batch_size,seq_len,self.d_model)\n",
        "    res = self.o(atten)\n",
        "    return res\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTh48ZAH8mSw"
      },
      "source": [
        "def test_MultiheadAttention():\n",
        "  batch_dim,seq_len,d_model= (15,10,200)\n",
        "  n_heads = 2\n",
        "  att = MultiheadAttention(d_model,causal_attention=False).to(device)\n",
        "  causal_att = MultiheadAttention(d_model,causal_attention=True).to(device)\n",
        "  x = torch.randn(batch_dim, n_heads, seq_len,3,d_model//n_heads).to(device)\n",
        "  q,k,v = x[:,:,:,0,:],x[:,:,:,1,:],x[:,:,:,2,:]\n",
        "  o1 = att(q,k,v)\n",
        "  o2 = causal_att(q,k,v)\n",
        "  assert o1.shape ==  torch.Size([batch_dim, seq_len,d_model])\n",
        "  assert o2.shape ==  torch.Size([batch_dim,  seq_len,d_model])\n",
        "test_MultiheadAttention()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTC9-skP2ory"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  '''\n",
        "  This class implements encoder block\n",
        "  '''\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(EncoderBlock,self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.qkv =  QKV(n_heads=n_heads,d_model=d_model)\n",
        "    self.attention = MultiheadAttention(d_model=d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = self.feedforward(x2)\n",
        "    x4 = self.norm2(self.dropout(x3)+x2)\n",
        "    return x4\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyWmFwuHFO46"
      },
      "source": [
        "def reshape_tensor(x,n_heads):\n",
        "  '''\n",
        "    Args:\n",
        "      x: tensor of shape (batch_size,seq_len,d_model)\n",
        "      n_heads: number of heads in mutlihead attention\n",
        "    Returns:\n",
        "      reshaped tensor of shape (batch_size,n_heads,seq_len,d_model//n_heads)    \n",
        "  '''\n",
        "  batch_size,seq_len,d_model = x.shape\n",
        "  x = x.reshape(batch_size,seq_len,n_heads,d_model//n_heads)\n",
        "  x = x.permute(0,2,1,3)\n",
        "  return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  '''\n",
        "    This class implements decoder block\n",
        "  '''\n",
        "\n",
        "  def __init__(self,d_model, n_heads, dim_feedforward, dropout=0.0):\n",
        "    '''\n",
        "      Args:\n",
        "        d_model: hidden space dimensions\n",
        "        n_heads: number of heads\n",
        "        dim_feedforward: Dimensionality of the hidden layer in the MLP  \n",
        "        drop: probability of an element to be zeroed\n",
        "    '''\n",
        "    super(DecoderBlock,self).__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "    self.qkv = QKV(n_heads,d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.causal_attention = MultiheadAttention(d_model,causal_attention=True)\n",
        "    self.feedforward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,x0,skip_con):\n",
        "    q,k,v = self.qkv(x0)\n",
        "    x1 = self.causal_attention(q,k,v)\n",
        "    x2 = self.norm1(x0+self.dropout(x1))\n",
        "    x3 = reshape_tensor(x2,self.n_heads)\n",
        "    skip_con = reshape_tensor(skip_con,self.n_heads)\n",
        "    x4 = self.attention(x3,skip_con,skip_con)\n",
        "    x5 = self.norm2(x2+self.dropout(x4))\n",
        "    x6 = self.feedforward(x5)\n",
        "    x7 = self.norm3(self.dropout(x6)+x5)\n",
        "    return x7"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUG1NMhDKAvw"
      },
      "source": [
        "class UnetTransformer(nn.Module):\n",
        "  '''\n",
        "    This class implements unet transformer\n",
        "  '''\n",
        "  def __init__(self,n_blocks,n_tokens,n_heads,d_model,dim_feedforward,emb_dropout=0.2,\\\n",
        "               out_dropout = 0.4, dropout=0.2):\n",
        "\n",
        "    '''\n",
        "      Args:\n",
        "        n_blocks: number of encoder/decoder blocks\n",
        "        n_tokens: Dimensionality of the input space\n",
        "        n_heads: number of heads in MultiHeadAttention\n",
        "        d_model: Dimensionality of the embedding space\n",
        "        num_classes: Dimensionality of the output space\n",
        "        dim_feedforward:  Dimensionality of the hidden layer in the MLP \n",
        "\n",
        "\n",
        "    '''\n",
        "    super(UnetTransformer,self).__init__()\n",
        "    self.n_blocks = n_blocks\n",
        "    self.emb = nn.Embedding(n_tokens, d_model)\n",
        "    self.pos_enc = PositionalEncoding( d_model=d_model, dropout = emb_dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['enc_'+str(i)] = EncoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    for i in range(n_blocks):\n",
        "      vars(self)['_modules']['dec_'+str(i)] = DecoderBlock(d_model, n_heads, dim_feedforward, dropout)\n",
        "    self.output_layer = nn.Sequential( nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(out_dropout),\n",
        "            nn.Linear(d_model, n_tokens)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "    x_encoded = self.pos_enc(self.emb(x))\n",
        "    layers = vars(self)['_modules']\n",
        "    stack = [x_encoded]\n",
        "    x = layers['enc_0'](x_encoded)\n",
        "    for i in range(1,self.n_blocks):\n",
        "      stack.append(x)\n",
        "      x = layers['enc_'+str(i)](x)\n",
        "    stack.append(x)\n",
        "    x = layers['dec_0'](x,stack.pop(0))\n",
        "    for i in range(1,self.n_blocks):\n",
        "      x = layers['dec_'+str(i)](x,stack.pop(0))\n",
        "    return self.output_layer(x)\n",
        "\n",
        "    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import dataset"
      ],
      "metadata": {
        "id": "BcWK--CdaBXU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "train_iter = WikiText2(split='train')\n",
        "tokenizer = get_tokenizer('spacy')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "train_iter, val_iter, test_iter = WikiText2()\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "c2n815-3aGA4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(source: Tensor, i: int, desired_len: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "\n",
        "    seq_len = min(desired_len, source.shape[1] - 1 - i)\n",
        "    data = source[:,i:i+seq_len]\n",
        "    target = source[:,i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target\n",
        "get_batch(train_data,0,10)[0].shape"
      ],
      "metadata": {
        "id": "0CGLbgM1aS2m",
        "outputId": "0f603fcb-681c-418f-d5c2-c9a280d024dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)\n",
        "print('n_tokens {}'.format(len(vocab)))\n",
        "model = UnetTransformer(n_blocks=n_blocks,n_tokens=ntokens,\\\n",
        "                        n_heads=n_heads, d_model = d_model,dim_feedforward = h_dims,\\\n",
        "                        dropout=dropout,out_dropout = out_dropout, emb_dropout=emb_dropout).to(device)\n",
        "pytorch_total_params = sum(p.numel()\n",
        "                        for p in model.parameters() if p.requires_grad)\n",
        "print('-' * 89)\n",
        "print(\n",
        "    '#'*12+f\" Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters for {epochs:3d} epochs \"+'#'*12)\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8us_WwAIaup5",
        "outputId": "c33e05e4-c88d-41bc-bd3f-62e6c134eac0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_tokens 33243\n",
            "-----------------------------------------------------------------------------------------\n",
            "############ Training model with 32.96M trainable parameters for 100 epochs ############\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicOpt:\n",
        "    def __init__(self, optimizer, schedular):\n",
        "        self.optimizer = optimizer\n",
        "        self.schedular = schedular\n",
        "        self._scalar = 1\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    @property\n",
        "    def lr(self):\n",
        "      return self.optimizer.param_groups[0]['lr']*self._scalar\n",
        "\n",
        "    @property\n",
        "    def scalar(self):\n",
        "        return self._scalar\n",
        "\n",
        "    @scalar.setter\n",
        "    def scalar(self, scalar):\n",
        "        self._scalar = scalar\n",
        "\n",
        "\n",
        "    def schedule_step(self, val_loss):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def step(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class linearcycleWarmup(BasicOpt):\n",
        "    def __init__(self, optimizer, schedular, *args, **kwargs):\n",
        "        super().__init__(optimizer=optimizer, schedular=schedular)\n",
        "        self.use_scheduler = True\n",
        "\n",
        "       \n",
        "    def step(self):\n",
        "        lr_s = [p['lr'] for p in self.optimizer.param_groups]\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr']  = p['lr']*self._scalar             \n",
        "        self.optimizer.step()\n",
        "        for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = lr_s[idx]\n",
        "        try:\n",
        "          if self.use_scheduler:\n",
        "            self.schedular.step()\n",
        "        except Exception as e:\n",
        "          self.use_scheduler = False\n",
        "          for idx, p in enumerate(self.optimizer.param_groups):\n",
        "            p['lr'] = 0.0000088\n",
        "\n",
        "\n",
        "\n",
        "    def schedule_step(self, *args):\n",
        "        pass\n",
        "steps_per_epoch = train_data.size(1)//bptt+1\n",
        "total_steps = epochs*(steps_per_epoch)\n",
        "opt_args = {\n",
        "    'lr': 0,\n",
        "    'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "}\n",
        "\n",
        "linear_args = {\n",
        "    'total_steps': total_steps,\n",
        "    'pct_start': 0.3, 'anneal_strategy': 'linear',\n",
        "    'three_phase': True, 'max_lr': 1e-3\n",
        "}\n",
        "opt = torch.optim.RAdam(model.parameters(),\n",
        "                        **opt_args)\n",
        "schedular_args = linear_args\n",
        "schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)"
      ],
      "metadata": {
        "id": "yk9NtX6gf9qw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sequence_length(bptt, use_var_length):\n",
        "    if not use_var_length:\n",
        "        return bptt\n",
        "    seq_len = bptt if np.random.random() < 0.95 else bptt // 2\n",
        "    seq_len = max(5, int(np.random.normal(seq_len, 5)))\n",
        "    seq_len = min(seq_len, bptt + 30)\n",
        "    return seq_len"
      ],
      "metadata": {
        "id": "Ph4OuVfI8gG5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = train_data.shape[1] // bptt\n",
        "    i, batch = 0, 0\n",
        "    while i < train_data.size(1) - 1 - 1:\n",
        "        data, targets = get_batch(train_data, i, get_sequence_length(bptt, use_var_length=True))\n",
        "        batch_size = data.size(1)\n",
        "        # if batch_size != bptt:  # only on last batch\n",
        "        #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data)\n",
        "        scale_lr = batch_size/bptt\n",
        "        optimizer.scalar = scale_lr\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = optimizer.lr\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.6f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}| seq_len {batch_size:5d}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        batch += 1\n",
        "        i += batch_size\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    #src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(1) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i, bptt)\n",
        "            batch_size = data.size(1)\n",
        "            # if batch_size != bptt:\n",
        "            #     src_mask = src_mask[:batch_size, :batch_size]\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
        "    return total_loss / (eval_data.size(1) - 1)"
      ],
      "metadata": {
        "id": "Gspv87NsaWha"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IHvZbtTJaa-4",
        "outputId": "71ee4df5-b85a-4c0b-a332-8da7f4e57336"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  434 batches | lr 0.000054 | ms/batch 116.39 | loss  9.33 | ppl 11303.54| seq_len   253\n",
            "| epoch   1 |   400/  434 batches | lr 0.000068 | ms/batch 114.87 | loss  7.28 | ppl  1455.50| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 52.87s | valid loss  5.95 | valid ppl   382.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  434 batches | lr 0.000084 | ms/batch 118.27 | loss  6.46 | ppl   640.18| seq_len   247\n",
            "| epoch   2 |   400/  434 batches | lr 0.000102 | ms/batch 112.92 | loss  6.25 | ppl   516.36| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 53.18s | valid loss  5.53 | valid ppl   252.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  434 batches | lr 0.000119 | ms/batch 114.95 | loss  6.08 | ppl   438.38| seq_len   254\n",
            "| epoch   3 |   400/  434 batches | lr 0.000134 | ms/batch 114.13 | loss  5.93 | ppl   375.38| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 52.81s | valid loss  5.32 | valid ppl   203.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  434 batches | lr 0.000156 | ms/batch 115.30 | loss  5.83 | ppl   339.40| seq_len   260\n",
            "| epoch   4 |   400/  434 batches | lr 0.000167 | ms/batch 113.44 | loss  5.72 | ppl   304.62| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 52.80s | valid loss  5.17 | valid ppl   176.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  434 batches | lr 0.000189 | ms/batch 114.05 | loss  5.66 | ppl   287.02| seq_len   260\n",
            "| epoch   5 |   400/  434 batches | lr 0.000193 | ms/batch 114.89 | loss  5.57 | ppl   261.55| seq_len   246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 52.91s | valid loss  5.07 | valid ppl   159.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  434 batches | lr 0.000112 | ms/batch 114.55 | loss  5.52 | ppl   250.24| seq_len   131\n",
            "| epoch   6 |   400/  434 batches | lr 0.000232 | ms/batch 113.04 | loss  5.45 | ppl   232.61| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 52.81s | valid loss  4.99 | valid ppl   147.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  434 batches | lr 0.000248 | ms/batch 115.89 | loss  5.41 | ppl   223.79| seq_len   252\n",
            "| epoch   7 |   400/  434 batches | lr 0.000265 | ms/batch 116.13 | loss  5.34 | ppl   208.78| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 52.89s | valid loss  4.92 | valid ppl   136.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  434 batches | lr 0.000292 | ms/batch 115.39 | loss  5.32 | ppl   203.56| seq_len   262\n",
            "| epoch   8 |   400/  434 batches | lr 0.000303 | ms/batch 113.50 | loss  5.26 | ppl   192.11| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 52.73s | valid loss  4.86 | valid ppl   128.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  434 batches | lr 0.000316 | ms/batch 112.98 | loss  5.24 | ppl   189.22| seq_len   255\n",
            "| epoch   9 |   400/  434 batches | lr 0.000330 | ms/batch 113.30 | loss  5.19 | ppl   179.57| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 52.75s | valid loss  4.81 | valid ppl   122.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  434 batches | lr 0.000355 | ms/batch 115.93 | loss  5.17 | ppl   176.60| seq_len   259\n",
            "| epoch  10 |   400/  434 batches | lr 0.000369 | ms/batch 114.62 | loss  5.13 | ppl   169.25| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 52.75s | valid loss  4.76 | valid ppl   116.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/  434 batches | lr 0.000375 | ms/batch 115.42 | loss  5.13 | ppl   168.29| seq_len   250\n",
            "| epoch  11 |   400/  434 batches | lr 0.000387 | ms/batch 113.45 | loss  5.09 | ppl   161.82| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 52.77s | valid loss  4.73 | valid ppl   113.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/  434 batches | lr 0.000410 | ms/batch 115.65 | loss  5.08 | ppl   160.81| seq_len   252\n",
            "| epoch  12 |   400/  434 batches | lr 0.000433 | ms/batch 114.28 | loss  5.04 | ppl   155.23| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 52.85s | valid loss  4.69 | valid ppl   109.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/  434 batches | lr 0.000446 | ms/batch 115.85 | loss  5.05 | ppl   155.99| seq_len   254\n",
            "| epoch  13 |   400/  434 batches | lr 0.000466 | ms/batch 114.19 | loss  5.01 | ppl   150.57| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 52.73s | valid loss  4.68 | valid ppl   107.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/  434 batches | lr 0.000461 | ms/batch 113.43 | loss  5.02 | ppl   152.05| seq_len   245\n",
            "| epoch  14 |   400/  434 batches | lr 0.000493 | ms/batch 115.65 | loss  4.99 | ppl   147.14| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 52.77s | valid loss  4.66 | valid ppl   105.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/  434 batches | lr 0.000505 | ms/batch 115.08 | loss  5.00 | ppl   148.13| seq_len   251\n",
            "| epoch  15 |   400/  434 batches | lr 0.000521 | ms/batch 113.86 | loss  4.97 | ppl   144.22| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 52.76s | valid loss  4.63 | valid ppl   102.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/  434 batches | lr 0.000557 | ms/batch 113.85 | loss  4.97 | ppl   144.27| seq_len   260\n",
            "| epoch  16 |   400/  434 batches | lr 0.000556 | ms/batch 115.20 | loss  4.94 | ppl   139.99| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 52.80s | valid loss  4.61 | valid ppl   100.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/  434 batches | lr 0.000588 | ms/batch 114.35 | loss  4.94 | ppl   139.61| seq_len   259\n",
            "| epoch  17 |   400/  434 batches | lr 0.000603 | ms/batch 113.71 | loss  4.91 | ppl   136.23| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 52.84s | valid loss  4.58 | valid ppl    97.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/  434 batches | lr 0.000614 | ms/batch 114.39 | loss  4.91 | ppl   135.57| seq_len   256\n",
            "| epoch  18 |   400/  434 batches | lr 0.000629 | ms/batch 114.21 | loss  4.88 | ppl   131.38| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 52.73s | valid loss  4.55 | valid ppl    94.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/  434 batches | lr 0.000639 | ms/batch 114.27 | loss  4.88 | ppl   130.99| seq_len   253\n",
            "| epoch  19 |   400/  434 batches | lr 0.000643 | ms/batch 113.04 | loss  4.85 | ppl   127.40| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 52.78s | valid loss  4.53 | valid ppl    92.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/  434 batches | lr 0.000693 | ms/batch 114.90 | loss  4.84 | ppl   126.57| seq_len   261\n",
            "| epoch  20 |   400/  434 batches | lr 0.000681 | ms/batch 113.01 | loss  4.81 | ppl   123.27| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 52.84s | valid loss  4.50 | valid ppl    89.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/  434 batches | lr 0.000707 | ms/batch 117.35 | loss  4.80 | ppl   121.96| seq_len   254\n",
            "| epoch  21 |   400/  434 batches | lr 0.000725 | ms/batch 113.53 | loss  4.78 | ppl   119.02| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 52.88s | valid loss  4.48 | valid ppl    88.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/  434 batches | lr 0.000757 | ms/batch 116.05 | loss  4.78 | ppl   118.61| seq_len   260\n",
            "| epoch  22 |   400/  434 batches | lr 0.000733 | ms/batch 113.73 | loss  4.76 | ppl   116.30| seq_len   247\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 53.07s | valid loss  4.45 | valid ppl    85.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/  434 batches | lr 0.000760 | ms/batch 114.99 | loss  4.75 | ppl   115.70| seq_len   250\n",
            "| epoch  23 |   400/  434 batches | lr 0.000787 | ms/batch 114.87 | loss  4.73 | ppl   113.18| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 53.22s | valid loss  4.42 | valid ppl    83.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/  434 batches | lr 0.000833 | ms/batch 116.73 | loss  4.72 | ppl   112.73| seq_len   263\n",
            "| epoch  24 |   400/  434 batches | lr 0.000842 | ms/batch 114.07 | loss  4.71 | ppl   111.02| seq_len   261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 52.99s | valid loss  4.40 | valid ppl    81.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/  434 batches | lr 0.000824 | ms/batch 115.47 | loss  4.71 | ppl   110.78| seq_len   250\n",
            "| epoch  25 |   400/  434 batches | lr 0.000842 | ms/batch 114.63 | loss  4.69 | ppl   108.75| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 52.92s | valid loss  4.39 | valid ppl    80.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/  434 batches | lr 0.000818 | ms/batch 115.51 | loss  4.69 | ppl   108.58| seq_len   239\n",
            "| epoch  26 |   400/  434 batches | lr 0.000860 | ms/batch 114.72 | loss  4.67 | ppl   107.08| seq_len   247\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 52.77s | valid loss  4.37 | valid ppl    79.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/  434 batches | lr 0.000885 | ms/batch 114.43 | loss  4.67 | ppl   106.66| seq_len   249\n",
            "| epoch  27 |   400/  434 batches | lr 0.000910 | ms/batch 114.74 | loss  4.66 | ppl   105.46| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 52.87s | valid loss  4.36 | valid ppl    78.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/  434 batches | lr 0.000950 | ms/batch 115.49 | loss  4.65 | ppl   104.92| seq_len   258\n",
            "| epoch  28 |   400/  434 batches | lr 0.000994 | ms/batch 115.55 | loss  4.64 | ppl   103.74| seq_len   266\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 52.81s | valid loss  4.35 | valid ppl    77.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/  434 batches | lr 0.000964 | ms/batch 115.88 | loss  4.64 | ppl   103.45| seq_len   253\n",
            "| epoch  29 |   400/  434 batches | lr 0.000967 | ms/batch 112.72 | loss  4.63 | ppl   102.72| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 52.79s | valid loss  4.34 | valid ppl    76.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/  434 batches | lr 0.000992 | ms/batch 113.97 | loss  4.63 | ppl   102.46| seq_len   256\n",
            "| epoch  30 |   400/  434 batches | lr 0.001000 | ms/batch 111.86 | loss  4.62 | ppl   101.92| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 52.76s | valid loss  4.33 | valid ppl    76.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/  434 batches | lr 0.000951 | ms/batch 113.94 | loss  4.61 | ppl   100.35| seq_len   254\n",
            "| epoch  31 |   400/  434 batches | lr 0.000918 | ms/batch 113.54 | loss  4.60 | ppl    99.34| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 52.74s | valid loss  4.32 | valid ppl    75.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/  434 batches | lr 0.000926 | ms/batch 115.13 | loss  4.58 | ppl    97.70| seq_len   256\n",
            "| epoch  32 |   400/  434 batches | lr 0.000427 | ms/batch 113.70 | loss  4.57 | ppl    96.48| seq_len   120\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 52.85s | valid loss  4.31 | valid ppl    74.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/  434 batches | lr 0.000910 | ms/batch 116.29 | loss  4.56 | ppl    95.48| seq_len   261\n",
            "| epoch  33 |   400/  434 batches | lr 0.000844 | ms/batch 113.99 | loss  4.54 | ppl    94.06| seq_len   246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 52.74s | valid loss  4.31 | valid ppl    74.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/  434 batches | lr 0.000863 | ms/batch 115.18 | loss  4.54 | ppl    93.32| seq_len   257\n",
            "| epoch  34 |   400/  434 batches | lr 0.000878 | ms/batch 114.10 | loss  4.53 | ppl    92.41| seq_len   266\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 52.80s | valid loss  4.28 | valid ppl    72.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/  434 batches | lr 0.000830 | ms/batch 115.18 | loss  4.52 | ppl    91.57| seq_len   257\n",
            "| epoch  35 |   400/  434 batches | lr 0.000813 | ms/batch 112.53 | loss  4.50 | ppl    90.17| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 52.69s | valid loss  4.27 | valid ppl    71.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/  434 batches | lr 0.000791 | ms/batch 114.63 | loss  4.49 | ppl    89.36| seq_len   255\n",
            "| epoch  36 |   400/  434 batches | lr 0.000780 | ms/batch 113.49 | loss  4.48 | ppl    88.31| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 52.82s | valid loss  4.26 | valid ppl    71.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/  434 batches | lr 0.000752 | ms/batch 115.33 | loss  4.47 | ppl    87.64| seq_len   253\n",
            "| epoch  37 |   400/  434 batches | lr 0.000738 | ms/batch 115.19 | loss  4.46 | ppl    86.63| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 52.81s | valid loss  4.26 | valid ppl    70.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/  434 batches | lr 0.000714 | ms/batch 115.56 | loss  4.45 | ppl    85.83| seq_len   251\n",
            "| epoch  38 |   400/  434 batches | lr 0.000731 | ms/batch 113.28 | loss  4.45 | ppl    85.26| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 52.84s | valid loss  4.25 | valid ppl    70.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/  434 batches | lr 0.000715 | ms/batch 115.33 | loss  4.44 | ppl    84.43| seq_len   263\n",
            "| epoch  39 |   400/  434 batches | lr 0.000654 | ms/batch 114.94 | loss  4.42 | ppl    83.34| seq_len   246\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 52.87s | valid loss  4.25 | valid ppl    69.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/  434 batches | lr 0.000660 | ms/batch 115.56 | loss  4.41 | ppl    82.63| seq_len   255\n",
            "| epoch  40 |   400/  434 batches | lr 0.000656 | ms/batch 113.96 | loss  4.40 | ppl    81.74| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 52.82s | valid loss  4.24 | valid ppl    69.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/  434 batches | lr 0.000647 | ms/batch 116.06 | loss  4.40 | ppl    81.57| seq_len   263\n",
            "| epoch  41 |   400/  434 batches | lr 0.000598 | ms/batch 113.98 | loss  4.39 | ppl    80.64| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 52.83s | valid loss  4.24 | valid ppl    69.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/  434 batches | lr 0.000583 | ms/batch 115.37 | loss  4.38 | ppl    79.70| seq_len   250\n",
            "| epoch  42 |   400/  434 batches | lr 0.000571 | ms/batch 113.98 | loss  4.37 | ppl    79.01| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 52.81s | valid loss  4.22 | valid ppl    68.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/  434 batches | lr 0.000554 | ms/batch 114.59 | loss  4.37 | ppl    78.70| seq_len   251\n",
            "| epoch  43 |   400/  434 batches | lr 0.000556 | ms/batch 111.64 | loss  4.36 | ppl    78.14| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 52.72s | valid loss  4.21 | valid ppl    67.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/  434 batches | lr 0.000531 | ms/batch 114.58 | loss  4.35 | ppl    77.64| seq_len   256\n",
            "| epoch  44 |   400/  434 batches | lr 0.000511 | ms/batch 115.03 | loss  4.34 | ppl    77.04| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 52.75s | valid loss  4.21 | valid ppl    67.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/  434 batches | lr 0.000485 | ms/batch 113.58 | loss  4.34 | ppl    76.90| seq_len   249\n",
            "| epoch  45 |   400/  434 batches | lr 0.000480 | ms/batch 113.57 | loss  4.33 | ppl    76.02| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 52.72s | valid loss  4.21 | valid ppl    67.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/  434 batches | lr 0.000460 | ms/batch 115.69 | loss  4.32 | ppl    75.24| seq_len   253\n",
            "| epoch  46 |   400/  434 batches | lr 0.000439 | ms/batch 112.81 | loss  4.31 | ppl    74.61| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 52.77s | valid loss  4.20 | valid ppl    66.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/  434 batches | lr 0.000434 | ms/batch 113.87 | loss  4.31 | ppl    74.66| seq_len   257\n",
            "| epoch  47 |   400/  434 batches | lr 0.000418 | ms/batch 113.43 | loss  4.30 | ppl    73.62| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 52.67s | valid loss  4.19 | valid ppl    66.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/  434 batches | lr 0.000411 | ms/batch 113.99 | loss  4.29 | ppl    72.90| seq_len   263\n",
            "| epoch  48 |   400/  434 batches | lr 0.000383 | ms/batch 115.59 | loss  4.28 | ppl    72.19| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 52.74s | valid loss  4.19 | valid ppl    66.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/  434 batches | lr 0.000368 | ms/batch 115.32 | loss  4.27 | ppl    71.63| seq_len   257\n",
            "| epoch  49 |   400/  434 batches | lr 0.000352 | ms/batch 114.51 | loss  4.26 | ppl    71.05| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 52.74s | valid loss  4.18 | valid ppl    65.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/  434 batches | lr 0.000334 | ms/batch 115.72 | loss  4.26 | ppl    70.64| seq_len   256\n",
            "| epoch  50 |   400/  434 batches | lr 0.000321 | ms/batch 113.33 | loss  4.25 | ppl    70.27| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 52.75s | valid loss  4.17 | valid ppl    64.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   200/  434 batches | lr 0.000152 | ms/batch 114.91 | loss  4.24 | ppl    69.63| seq_len   129\n",
            "| epoch  51 |   400/  434 batches | lr 0.000140 | ms/batch 113.85 | loss  4.24 | ppl    69.35| seq_len   125\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 52.79s | valid loss  4.17 | valid ppl    64.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   200/  434 batches | lr 0.000272 | ms/batch 115.73 | loss  4.22 | ppl    68.15| seq_len   259\n",
            "| epoch  52 |   400/  434 batches | lr 0.000258 | ms/batch 113.84 | loss  4.23 | ppl    68.38| seq_len   260\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 52.76s | valid loss  4.17 | valid ppl    64.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |   200/  434 batches | lr 0.000228 | ms/batch 114.60 | loss  4.21 | ppl    67.22| seq_len   247\n",
            "| epoch  53 |   400/  434 batches | lr 0.000219 | ms/batch 114.00 | loss  4.21 | ppl    67.57| seq_len   253\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 52.88s | valid loss  4.16 | valid ppl    63.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   200/  434 batches | lr 0.000211 | ms/batch 113.99 | loss  4.20 | ppl    66.70| seq_len   266\n",
            "| epoch  54 |   400/  434 batches | lr 0.000190 | ms/batch 114.40 | loss  4.20 | ppl    66.42| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 52.77s | valid loss  4.15 | valid ppl    63.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |   200/  434 batches | lr 0.000169 | ms/batch 114.09 | loss  4.18 | ppl    65.67| seq_len   255\n",
            "| epoch  55 |   400/  434 batches | lr 0.000155 | ms/batch 112.44 | loss  4.18 | ppl    65.41| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 52.70s | valid loss  4.15 | valid ppl    63.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |   200/  434 batches | lr 0.000136 | ms/batch 114.41 | loss  4.17 | ppl    64.57| seq_len   255\n",
            "| epoch  56 |   400/  434 batches | lr 0.000125 | ms/batch 114.81 | loss  4.16 | ppl    64.34| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 52.76s | valid loss  4.15 | valid ppl    63.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |   200/  434 batches | lr 0.000105 | ms/batch 114.23 | loss  4.15 | ppl    63.57| seq_len   258\n",
            "| epoch  57 |   400/  434 batches | lr 0.000087 | ms/batch 115.22 | loss  4.15 | ppl    63.41| seq_len   250\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 52.83s | valid loss  4.14 | valid ppl    62.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |   200/  434 batches | lr 0.000075 | ms/batch 113.85 | loss  4.14 | ppl    62.80| seq_len   270\n",
            "| epoch  58 |   400/  434 batches | lr 0.000057 | ms/batch 112.58 | loss  4.14 | ppl    62.93| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 52.73s | valid loss  4.13 | valid ppl    62.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |   200/  434 batches | lr 0.000040 | ms/batch 114.86 | loss  4.13 | ppl    62.16| seq_len   257\n",
            "| epoch  59 |   400/  434 batches | lr 0.000040 | ms/batch 114.45 | loss  4.13 | ppl    62.10| seq_len   261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 52.69s | valid loss  4.13 | valid ppl    62.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |   200/  434 batches | lr 0.000040 | ms/batch 114.03 | loss  4.12 | ppl    61.61| seq_len   261\n",
            "| epoch  60 |   400/  434 batches | lr 0.000038 | ms/batch 114.96 | loss  4.12 | ppl    61.70| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 52.76s | valid loss  4.13 | valid ppl    61.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |   200/  434 batches | lr 0.000038 | ms/batch 115.35 | loss  4.11 | ppl    61.02| seq_len   255\n",
            "| epoch  61 |   400/  434 batches | lr 0.000038 | ms/batch 114.67 | loss  4.11 | ppl    61.18| seq_len   263\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 52.79s | valid loss  4.13 | valid ppl    62.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |   200/  434 batches | lr 0.000038 | ms/batch 115.85 | loss  4.11 | ppl    60.71| seq_len   261\n",
            "| epoch  62 |   400/  434 batches | lr 0.000037 | ms/batch 114.37 | loss  4.11 | ppl    61.04| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 52.92s | valid loss  4.13 | valid ppl    61.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |   200/  434 batches | lr 0.000036 | ms/batch 116.75 | loss  4.10 | ppl    60.53| seq_len   256\n",
            "| epoch  63 |   400/  434 batches | lr 0.000034 | ms/batch 114.80 | loss  4.10 | ppl    60.61| seq_len   249\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 53.02s | valid loss  4.13 | valid ppl    61.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |   200/  434 batches | lr 0.000036 | ms/batch 115.24 | loss  4.10 | ppl    60.15| seq_len   265\n",
            "| epoch  64 |   400/  434 batches | lr 0.000034 | ms/batch 114.36 | loss  4.10 | ppl    60.34| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 52.93s | valid loss  4.12 | valid ppl    61.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |   200/  434 batches | lr 0.000018 | ms/batch 116.29 | loss  4.09 | ppl    59.75| seq_len   133\n",
            "| epoch  65 |   400/  434 batches | lr 0.000034 | ms/batch 115.79 | loss  4.09 | ppl    59.99| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 53.16s | valid loss  4.12 | valid ppl    61.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |   200/  434 batches | lr 0.000016 | ms/batch 113.65 | loss  4.09 | ppl    59.55| seq_len   128\n",
            "| epoch  66 |   400/  434 batches | lr 0.000033 | ms/batch 112.01 | loss  4.10 | ppl    60.18| seq_len   258\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 52.71s | valid loss  4.12 | valid ppl    61.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |   200/  434 batches | lr 0.000015 | ms/batch 114.35 | loss  4.08 | ppl    59.34| seq_len   125\n",
            "| epoch  67 |   400/  434 batches | lr 0.000031 | ms/batch 114.17 | loss  4.09 | ppl    59.60| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 52.75s | valid loss  4.12 | valid ppl    61.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |   200/  434 batches | lr 0.000031 | ms/batch 114.00 | loss  4.08 | ppl    59.07| seq_len   261\n",
            "| epoch  68 |   400/  434 batches | lr 0.000029 | ms/batch 112.41 | loss  4.09 | ppl    59.52| seq_len   245\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 52.69s | valid loss  4.12 | valid ppl    61.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |   200/  434 batches | lr 0.000029 | ms/batch 114.33 | loss  4.07 | ppl    58.77| seq_len   253\n",
            "| epoch  69 |   400/  434 batches | lr 0.000031 | ms/batch 114.62 | loss  4.08 | ppl    59.23| seq_len   269\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 52.80s | valid loss  4.12 | valid ppl    61.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |   200/  434 batches | lr 0.000028 | ms/batch 113.76 | loss  4.07 | ppl    58.68| seq_len   252\n",
            "| epoch  70 |   400/  434 batches | lr 0.000028 | ms/batch 114.27 | loss  4.08 | ppl    59.11| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 52.75s | valid loss  4.12 | valid ppl    61.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |   200/  434 batches | lr 0.000027 | ms/batch 116.52 | loss  4.07 | ppl    58.50| seq_len   252\n",
            "| epoch  71 |   400/  434 batches | lr 0.000027 | ms/batch 113.54 | loss  4.07 | ppl    58.76| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 52.74s | valid loss  4.12 | valid ppl    61.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |   200/  434 batches | lr 0.000026 | ms/batch 115.68 | loss  4.07 | ppl    58.39| seq_len   252\n",
            "| epoch  72 |   400/  434 batches | lr 0.000027 | ms/batch 115.00 | loss  4.07 | ppl    58.66| seq_len   264\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 52.75s | valid loss  4.12 | valid ppl    61.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |   200/  434 batches | lr 0.000025 | ms/batch 114.58 | loss  4.06 | ppl    58.20| seq_len   250\n",
            "| epoch  73 |   400/  434 batches | lr 0.000025 | ms/batch 113.40 | loss  4.07 | ppl    58.56| seq_len   255\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 52.71s | valid loss  4.12 | valid ppl    61.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |   200/  434 batches | lr 0.000024 | ms/batch 113.51 | loss  4.07 | ppl    58.28| seq_len   246\n",
            "| epoch  74 |   400/  434 batches | lr 0.000024 | ms/batch 115.07 | loss  4.07 | ppl    58.29| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 52.85s | valid loss  4.12 | valid ppl    61.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |   200/  434 batches | lr 0.000023 | ms/batch 113.49 | loss  4.06 | ppl    58.07| seq_len   254\n",
            "| epoch  75 |   400/  434 batches | lr 0.000023 | ms/batch 115.15 | loss  4.06 | ppl    58.06| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 52.80s | valid loss  4.12 | valid ppl    61.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |   200/  434 batches | lr 0.000023 | ms/batch 113.72 | loss  4.06 | ppl    57.86| seq_len   257\n",
            "| epoch  76 |   400/  434 batches | lr 0.000022 | ms/batch 114.83 | loss  4.06 | ppl    58.00| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 52.87s | valid loss  4.12 | valid ppl    61.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |   200/  434 batches | lr 0.000021 | ms/batch 115.56 | loss  4.05 | ppl    57.60| seq_len   256\n",
            "| epoch  77 |   400/  434 batches | lr 0.000021 | ms/batch 114.20 | loss  4.06 | ppl    57.88| seq_len   254\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 52.84s | valid loss  4.12 | valid ppl    61.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |   200/  434 batches | lr 0.000020 | ms/batch 115.15 | loss  4.05 | ppl    57.67| seq_len   255\n",
            "| epoch  78 |   400/  434 batches | lr 0.000020 | ms/batch 114.24 | loss  4.06 | ppl    57.76| seq_len   252\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 52.82s | valid loss  4.12 | valid ppl    61.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |   200/  434 batches | lr 0.000018 | ms/batch 113.86 | loss  4.05 | ppl    57.64| seq_len   244\n",
            "| epoch  79 |   400/  434 batches | lr 0.000019 | ms/batch 112.97 | loss  4.06 | ppl    57.80| seq_len   257\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 52.78s | valid loss  4.12 | valid ppl    61.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |   200/  434 batches | lr 0.000018 | ms/batch 114.16 | loss  4.05 | ppl    57.52| seq_len   257\n",
            "| epoch  80 |   400/  434 batches | lr 0.000009 | ms/batch 115.95 | loss  4.05 | ppl    57.54| seq_len   134\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 52.89s | valid loss  4.12 | valid ppl    61.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |   200/  434 batches | lr 0.000017 | ms/batch 113.75 | loss  4.05 | ppl    57.56| seq_len   252\n",
            "| epoch  81 |   400/  434 batches | lr 0.000017 | ms/batch 114.80 | loss  4.05 | ppl    57.42| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 52.88s | valid loss  4.12 | valid ppl    61.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |   200/  434 batches | lr 0.000009 | ms/batch 114.06 | loss  4.05 | ppl    57.32| seq_len   136\n",
            "| epoch  82 |   400/  434 batches | lr 0.000008 | ms/batch 114.16 | loss  4.05 | ppl    57.20| seq_len   131\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 52.75s | valid loss  4.12 | valid ppl    61.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |   200/  434 batches | lr 0.000016 | ms/batch 116.61 | loss  4.04 | ppl    57.02| seq_len   263\n",
            "| epoch  83 |   400/  434 batches | lr 0.000015 | ms/batch 114.39 | loss  4.04 | ppl    57.08| seq_len   262\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 52.86s | valid loss  4.12 | valid ppl    61.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |   200/  434 batches | lr 0.000014 | ms/batch 115.27 | loss  4.04 | ppl    56.81| seq_len   259\n",
            "| epoch  84 |   400/  434 batches | lr 0.000014 | ms/batch 115.13 | loss  4.04 | ppl    56.98| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 52.73s | valid loss  4.12 | valid ppl    61.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |   200/  434 batches | lr 0.000013 | ms/batch 113.80 | loss  4.04 | ppl    56.89| seq_len   252\n",
            "| epoch  85 |   400/  434 batches | lr 0.000012 | ms/batch 113.85 | loss  4.04 | ppl    57.05| seq_len   245\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 52.73s | valid loss  4.11 | valid ppl    61.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |   200/  434 batches | lr 0.000013 | ms/batch 115.78 | loss  4.04 | ppl    56.82| seq_len   264\n",
            "| epoch  86 |   400/  434 batches | lr 0.000012 | ms/batch 114.77 | loss  4.04 | ppl    56.93| seq_len   251\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 52.72s | valid loss  4.11 | valid ppl    61.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |   200/  434 batches | lr 0.000011 | ms/batch 114.48 | loss  4.04 | ppl    56.72| seq_len   253\n",
            "| epoch  87 |   400/  434 batches | lr 0.000011 | ms/batch 116.52 | loss  4.04 | ppl    56.85| seq_len   264\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 52.95s | valid loss  4.11 | valid ppl    61.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |   200/  434 batches | lr 0.000010 | ms/batch 114.44 | loss  4.03 | ppl    56.53| seq_len   260\n",
            "| epoch  88 |   400/  434 batches | lr 0.000010 | ms/batch 114.63 | loss  4.04 | ppl    56.75| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 52.88s | valid loss  4.11 | valid ppl    61.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |   200/  434 batches | lr 0.000009 | ms/batch 113.71 | loss  4.03 | ppl    56.54| seq_len   260\n",
            "| epoch  89 |   400/  434 batches | lr 0.000009 | ms/batch 115.31 | loss  4.04 | ppl    56.68| seq_len   261\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 52.80s | valid loss  4.11 | valid ppl    61.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |   200/  434 batches | lr 0.000008 | ms/batch 114.97 | loss  4.03 | ppl    56.53| seq_len   257\n",
            "| epoch  90 |   400/  434 batches | lr 0.000008 | ms/batch 114.22 | loss  4.04 | ppl    56.68| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 52.85s | valid loss  4.11 | valid ppl    61.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |   200/  434 batches | lr 0.000007 | ms/batch 114.30 | loss  4.04 | ppl    56.61| seq_len   249\n",
            "| epoch  91 |   400/  434 batches | lr 0.000007 | ms/batch 113.81 | loss  4.04 | ppl    56.62| seq_len   259\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 52.80s | valid loss  4.11 | valid ppl    61.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |   200/  434 batches | lr 0.000006 | ms/batch 117.14 | loss  4.03 | ppl    56.08| seq_len   253\n",
            "| epoch  92 |   400/  434 batches | lr 0.000006 | ms/batch 117.17 | loss  4.03 | ppl    56.25| seq_len   256\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 53.18s | valid loss  4.11 | valid ppl    61.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |   200/  434 batches | lr 0.000005 | ms/batch 115.68 | loss  4.03 | ppl    56.30| seq_len   253\n",
            "| epoch  93 |   400/  434 batches | lr 0.000005 | ms/batch 113.88 | loss  4.04 | ppl    56.58| seq_len   254\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c9ddfe8311b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-5acd41d6dab6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-3f0316c281ae>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/radam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     eps=group['eps'])\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36mradam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, state_steps, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;31m# Compute the variance rectification term and update parameters accordingly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrho_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho_inf\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrho_inf\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrho_inf\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrho_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0madaptive_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_corrected_exp_avg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madaptive_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('-' * 89)\n",
        "print(f'test loss {test_loss:5.5f} | test ppl {test_ppl:8.5f}')\n",
        "print('-' * 89)"
      ],
      "metadata": {
        "id": "1Zgbkytp9Gqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "B9PEwV1-L9wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rep in range(10):\n",
        "  epochs = 10             \n",
        "  lr = 5e-4               \n",
        "  clip = 0.5\n",
        "  bptt = 512\n",
        "  # best_val_loss = float('inf')\n",
        "  # best_model = None\n",
        "  steps_per_epoch = train_data.size(1)//bptt+1\n",
        "  total_steps = epochs*(steps_per_epoch)\n",
        "  opt_args = {\n",
        "      'lr': 0,\n",
        "      'betas': (0.9, 0.98), 'eps': 1e-9, 'weight_decay': 1e-5\n",
        "  }\n",
        "\n",
        "  linear_args = {\n",
        "      'total_steps': total_steps,\n",
        "      'pct_start': 0.1, 'anneal_strategy': 'linear',\n",
        "      'three_phase': True, 'max_lr': 1e-3\n",
        "  }\n",
        "  opt = torch.optim.RAdam(model.parameters(),\n",
        "                          **opt_args)\n",
        "  schedular_args = linear_args\n",
        "  schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, **schedular_args)\n",
        "  optimizer = linearcycleWarmup(optimizer = opt, schedular=schedular )\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      epoch_start_time = time.time()\n",
        "      train(model)\n",
        "      val_loss = evaluate(model, val_data)\n",
        "      val_ppl = math.exp(val_loss)\n",
        "      elapsed = time.time() - epoch_start_time\n",
        "      print('-' * 89)\n",
        "      print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "      print('-' * 89)\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          del best_model\n",
        "          best_model = copy.deepcopy(model)"
      ],
      "metadata": {
        "id": "vxsIgBSvLhrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Not important"
      ],
      "metadata": {
        "id": "ON8LmbvYdMGU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndJCTLgBuQHp"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  num_classes = train_loader.dataset.num_categories\n",
        "  l = 0\n",
        "  acc = 0\n",
        "  pbar = tqdm(total = len(train_loader),position=0,leave=True)\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    x, output_shifted = data\n",
        "    preds = model(x.to(device), output_shifted.to(device))       \n",
        "    loss = criterion(preds.view(-1,preds.size(-1)), target.view(-1),reduction=\"mean\")\n",
        "    loss.backward() \n",
        "    #nn.utils.clip_grad_norm(model.parameters(), clip)       \n",
        "    optimizer.step()\n",
        "    with torch.no_grad():\n",
        "      current_loss = loss.item()\n",
        "      l+= loss.item()\n",
        "      acc+=(preds.argmax(dim=-1) == target).float().mean().item()\n",
        "    pbar.set_description('training_step {} loss:{:3f}'.format(batch_idx,current_loss))\n",
        "    pbar.update()\n",
        "  acc = 100. * acc / (len(train_loader))\n",
        "  l = l/len(train_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format('train',l,acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOzqXY-WwNlr"
      },
      "source": [
        "def test( model, device, test_loader,criterion,mode='eval'):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  num_classes=test_loader.dataset.num_categories\n",
        "  with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "          target = target.to(device)\n",
        "          x, output_shifted = data\n",
        "          output = model(x.to(device), output_shifted.to(device))\n",
        "          test_loss += criterion(output.view(-1,output.size(-1)),\\\n",
        "                                        target.view(-1)).item()          \n",
        "          correct += (output.argmax(dim=-1) == target).float().mean().item()\n",
        "\n",
        "  loss = test_loss/len(test_loader)\n",
        "  acc = 100. * correct / len(test_loader)\n",
        "  print('{0}: loss: {1:.3f} acc {2:.1f}'.format(mode,loss,acc))\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raohC_7iRk0N"
      },
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "\n",
        "        self.data = torch.randint(low=1, high=self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        labels_shifted = labels.roll(1,0)\n",
        "        labels_shifted[0] = torch.tensor(0)\n",
        "        return (inp_data,labels_shifted), labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ8jEEssRQX5"
      },
      "source": [
        "def main():\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.determinstic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    dataset = partial(ReverseDataset, 10, 16)\n",
        "    train_loader = data.DataLoader(dataset(50000), batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    \n",
        "    val_loader   = data.DataLoader(dataset(1000), batch_size=test_batch_size)\n",
        "    test_loader  = data.DataLoader(dataset(10000), batch_size=test_batch_size)\n",
        "    model = UnetTransformer(n_blocks=n_blocks,input_dim=train_loader.dataset.num_categories,\\\n",
        "                            n_heads=n_heads,d_model = h_dims,num_classes=\\\n",
        "                            train_loader.dataset.num_categories,dim_feedforward = h_dims,\\\n",
        "                            dropout=dropout).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "    criterion =  F.cross_entropy\n",
        "    \n",
        "\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "       train(model, device, train_loader, optimizer,criterion)\n",
        "       test(model, device, val_loader,criterion)\n",
        "        \n",
        "    torch.save(model.state_dict(), \"model.h5\")\n",
        "    print('------------testing--------------')\n",
        "    test(model, device, test_loader,criterion,mode='test')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n",
        "    #main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}